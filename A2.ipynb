{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemanta1212/UTS_ML2019_ID12863181/blob/master/A2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mEx0R5nE8UDy"
      },
      "source": [
        "# **Use of Cloud Computing Services for Machine Learning in Online Retail Data for Customer Segmentation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ak0Gdtg0L3yd"
      },
      "source": [
        "# **Introduction** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "21laKCdOMHjr"
      },
      "source": [
        "In past years, when customers face any issues, they used to approach some limited help lines and offices. With the changing era, in this digital world we have collected a lot of data whose size is getting increased day by day exponentially. With this collected data, can analyse the patterns, behaviour and changing trends.  Such analysis in any field is very useful for the targeted market as well as business owners.  Such analysis can help the business developers to understand the needs and patterns of the customer using machine learning models. The customer analytics can be divided into the various areas like the customer segmentation to identify types of customers, predicting customer actions like churn or purchases and understanding customer views based on surveys to learn about attitudes and expectations. In this age of data, have collected many tera bytes of data. Using many machine learning methods, this huge amount of data can be used to understand the past patterns of customers as well predict the future behaviour of customers. Machine learning has become an integral part of our lives, helping us to do everything from searching photos to driving cars. The main aim of machine learning is to allow the computers learn automatically without human assistance. With such huge amount of customer data, can do many analyses of the customer based data by machine learning algorithms where can train the algorithms with some data and then test the algorithm on remaining data. Using machine learning algorithms we find out the patterns of customer’s purchase, type of customer, loyalty of customers, needs and requirements of customer. There are many powerful platforms available to analyse customers’ data and apply such algorithms. Customer analytics using machine learning models can help business to grow and understand the needs, requirements, and patterns of customers. In this study we will do data mining such analysis on customer and purchase data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fg4z8z6VNdJz"
      },
      "source": [
        "**Customer Relation management**\n",
        "\n",
        "Customer Relationship Management (CRM) is an enterprise method that develops a complete knowledge of customer behavior and predilection. By developing a program and planning, it inspires the customer to improve their business relationship with the company. It is a strategy that is used by companies to organize and analyze customer data during the lifecycle of the customer. The data can be gathered through website of company, live chat, email, social media, and telephone calls.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3IC_YhM8Ng5d"
      },
      "source": [
        "**Importance and Applications of CRM**\n",
        "\n",
        "Importance and Applications of CRM facilitates the business holders to deepen their relationship with the customer, service users, merchants, colleagues, and partners. \n",
        "It is critical for customer acquisition and retention to maintain the good relationship and track the prospect and customer. You can see everything integrated at one place that can give information about client's history with company, order detail and more. According to the prediction of Garnetr in 2021 CRM will be the biggest revenue of enterprise software. Benefits of the CRM include: “Enhanced contact management”, “Cross-team association”, “Heightened productivity”, “Empowered sales management”, “Steadfast reporting”, “Enhanced sales metrics”, “enlarged customer satisfaction and retention”, “Enhanced marketing ROI”, “Enhanced products”.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UtiFMMGVNXu4"
      },
      "source": [
        "**Components of CRM**\n",
        "\n",
        "CRM software combines data of customer and documents in a stand-alone database that makes it easier for the business user to access and to manage it. With time, some other functions have been included in CRM system to make it more beneficial. These functions include: maintaining interaction with the customer using email, phone, social media and other channels. Some components of CRM are discussed below.  \n",
        "\n",
        "•\tMarketing automation\n",
        "\n",
        "Automation can be used to minimize the working efforts for marketing team. For example email can be sent to all customers for sales to minimize the other communication efforts.\n",
        "•\tSales force automation\n",
        " Sales force automation tool automate business function and sales cycle to target and find new clients. This tool track customer interaction with company. \n",
        "\n",
        "•\tContact center automation\n",
        "\n",
        "Pre-recorded audios can use to reduce the task of contact center agent. Some software tools are introduced that can be added with the desktop of the agent. These tools respond to a customer request in case of cut down during call and also make customer service process easier.\n",
        "\n",
        "•\tGeolocation technology or location-based services\n",
        "\n",
        "Some CRM provides location-based services that can make geographic marketing campaigns by tracking a customer’s physical location. These geolocation technologies are integrated with location tracking GPS applications. This technology also helps as contact management tool or networking tool by finding sales prospect according to location.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zCfPbbzlMHc-"
      },
      "source": [
        "\n",
        "**Cloud Computing Services**\n",
        "\n",
        "Cloud computing services include storage, servers, networking, databases, software analytics and intelligence over the internet that offer flexible resources and faster innovation. Machine learning systems were too costly and too complex for most enterprises in the past. The Cloud computing services are changing all that. The cloud computing services provide the high compute power, storage and GPU processing powers to manage the machine learning projects in a good way. There are several cloud computing service provider like Google collaboration, Amazon web services, Microsoft azure.  These cloud computing services will be used for customer segmentation for the first time in this field of research.  In previous studies, the researchers used the local machines for all the work on customer segmentation while this study going to use the cloud computing services. Analysis of customer segmentation data, preprocessing, grouping the data into clusters and classification of customer segmentation are all the main steps that will be done on cloud computing services. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OMsqGDkSOgxB"
      },
      "source": [
        "**Dataset**\n",
        "\n",
        "We used dataset of a non-store retailer store that sale all occasions gifts and most of their customers are whole sellers. We discussed the detailed characteristics of the dataset in below section and in next chapter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wSxDv0WYOtJh"
      },
      "source": [
        "**Purpose of Data Set**\n",
        "\n",
        "The dataset used in our study was presented in another study, in which analysis of customer was conducted for effective business understanding and customer-centric marketing.  By considering the “Recency”, “Frequency”, and “Monetary” (PFM) model as base, the customers were segmented using “K-Means” and “decision tree” algorithm.  Characteristics of the customers were identified during segmentation and recommendations were provided to consumer-centric market. We use this dataset in our study with the purpose to segment the customer’s behavior, and classifications.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3cT4tTrfOtDO"
      },
      "source": [
        "**Scope of Study**\n",
        "\n",
        "The focus of this data minig project work is to improve the accuracy of existing machine learning techniques for customer segmentation in field of CRM. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fF7aEUuZOs9z"
      },
      "source": [
        "**Objectives of Study**\n",
        "\n",
        "The main research goals of this study are;\n",
        "\n",
        "•\tExplore in detail the Customer Segmentation problem and the related baseline techniques.\n",
        "\n",
        "•\tExplore the new data analysis and data preprocessing techniques \n",
        "\n",
        "•\tCustomer segmentation with stage process, the one is to group the data into clusters and the second is to classify the customers from the clusters.  \n",
        "\n",
        "•\tAlgorithm development and testing on  cloud computing services \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Pq_qNb1COs3W"
      },
      "source": [
        "**Deliverables**\n",
        "\n",
        " Data Set & Meta data\n",
        " \n",
        "•\tMachine Learning Algorithms (Customer Segmentation & Market Basket Analysis)\n",
        "\n",
        "•\tVisualizations describing the results\n",
        "\n",
        "•\tReport with the details and comparison of machine learning models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7bwQW98ROsx6"
      },
      "source": [
        "**Millstones**\n",
        "\n",
        "•\tDataset collection\n",
        "\n",
        "•\tDataset cleansing and transformation\n",
        "\n",
        "•\tMachine learning model selection as per the analysis requirement\n",
        "\n",
        "•\tMachine learning model creation in local system/storage\n",
        "\n",
        "•\tResult analysis and comparison\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XuJr-NL0Oss0"
      },
      "source": [
        "**CRM challenges**\n",
        "\n",
        "It is challenging to deal with customer records that contain replicated or outdated data. To take benefits from CRM it is needed to spend time in cleaning customer data to abolish replicated and inadequate records.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "loiCYwP7Osn5"
      },
      "source": [
        "**Main Contributions**\n",
        "\n",
        "Contributions of this study are,\n",
        "\n",
        "•\tIn this study, we have explored the problem of Customer Relationship Management.\n",
        "\n",
        "•\tAnalysis of the data will be in depth to understand all the situations like purchase products, cancel orders, basket price etc. for the development of the better model. \n",
        "\n",
        "•\tIn previous studies, they used clustering or classification techniques for the customer segmentation but in this study, we are going to work on two stage customer segmentation, in first stage, will use clustering for grouping the data into different clusters and in second stage, will use that cluster groups for classifications. \n",
        "\n",
        "•\tIn this, a robust algorithm will be developed that will use clustering to group the different products into specific main categories/clusters and after that will classify the customer’s consumption habits according to their related clusters. \n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LKPdlsb3Oshr"
      },
      "source": [
        "# **Methods and System Design**\n",
        " \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RNIK2CGWOscY"
      },
      "source": [
        "**Data Source**\n",
        "\n",
        "The dataset was collected from an online retailer UK-based non-store registered business that own 80 staff members. This business was established in 1981 that basically sale gift for all occasions. Initially, the company was use the method of mailing and calls to coordinate with the customers. But at the time of the research that conducted in 2012the company maintained its own website that is why only two years of data was available in the study. The company is successful in maintaining the good relationships with many customers and carrying out many customers from all over United Kingdom and Europe. That is why the company had large volume of data about many customers. Company also used Amazon.co.uk to get more customers.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "U7ySOJzbOsXk"
      },
      "source": [
        "**Dataset Generation Process**\n",
        "\n",
        "The dataset of the company was extracted as it is from 01 December 2010 to 09 December 2011.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XRgAsQpJOsSc"
      },
      "source": [
        "**Dataset Importance**\n",
        "\n",
        "The dataset will be collected from UCI repository and all the required extraction, transformation and manipulation will be done through the machine learning models. The dataset includes the product and customer’s details.\n",
        "Customer analytics is a very important and critical component of any kind of business plans and strategies for the growth of the business at any stage. The understanding and clarity of target market is very important for any business. Understanding the needs and requirements of the customers is as useful as reaching to them.\n",
        " Customer analysis helps in choosing the methods to reach the target market and also helps in satisfying the needs of the target market according to their profiles and demographics. A customer analysis can be categorized in two parts, behavioural profiles of the customers and demographic profiles of the customers. A customer profile is a vital tool to help the business better understands current and potential customers in order to increase the sales and growth of the business. Behavioural profile of the customer includes the purchase process and patterns. For example, how frequently a customer purchase, what type of items they purchase, number of items they purchase, amount of the product which they purchase etc. Demographic profile of customer includes the gender, age, ethnicity and income details in order to create a demographic profile of customers. There are multiple use cases where customer analysis can be used:\n",
        " \n",
        "●\tIdentifying the types of customers\n",
        "\n",
        "●\tIdentifying the best and potential \n",
        "customers\n",
        "\n",
        "●\tPlan the retention of new and churn customers\n",
        "\n",
        "●\tInducing further buying from existing customers\n",
        "\n",
        "●\tEffective and profitable campaign planning\n",
        "\n",
        "●\tImprovement in customer services\n",
        "\n",
        "Customer segmentation is also called market segmentation. Customer segmentation is a division of different types of customers in a market in discrete groups. Market basket analysis is a method to uncover the associations between items by large companies, retailers and various supply chains. It works by looking for the combinations of items brought together. Various association rules are used to analyse retail basket to identify the rules discovered in transactions.\n",
        "How the result of clustering and prediction varies on both the cloud computing and local machine\n",
        "If there are any similarities in cloud and local machine learning models, what are the factors affecting them?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PkTqhJEBOsNf"
      },
      "source": [
        "**Dataset Characteristics**\n",
        "\n",
        "Dataset contain total 541909 transactions of customers. All customers that placed, cancel or rejected the orders are recorded in the dataset. All the transactions below 01 December 2010 and above 09 December 2011 exist in the dataset. Invoice Number is a numeric no of 6 digits and for those orders where invoice get cancelled, a c is placed at the start of the Invoice Number to indicate the cancelation. For example, a simple Invoice Number is 536398 and cancel transaction of this invoice would be 536398c. Stock Code is a 5 digit nominal value that is assigned to each unique product. For example, Stock Code “22632” indicates the product “Hand Warmer Red Polka Dot“ in dataset. The Description contain the name of the product item. The Quantity contains the no of quantity per transaction. Invoice Date indicate the date in which the transaction is conducted. Unit Price indicates the per unit price of the product. Customer ID is the 5 digit number assigned to each customer to uniquely identify it. Country is the name of the country where each of the customer lives or to where the dispatch has to be made. \n",
        "The below section discuss the algorithms applied in this study such as Support Vector Machine Classifier, Logistic Regression, K-Nearest Neighbors, Decision Tree, Random Forest, AdaBoost and Gradient Boosting Classifier. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nt9sj4-GOsHW"
      },
      "source": [
        "**Cloud Computing Services**\n",
        "\n",
        "The detailed discussion have been done in introduction chapter regarding cloud computing services. In this study, Google Colaboratory, Amazon Web Services and Microsoft Azure notebooks will be used for the analysis of the dataset, preprocessing of the dataset, clustering of the dataset and finally the classification of customers segmentation with the use of above discussed machine learning algorithm. In previous studies, the researchers used the local machines for all the work on customer segmentation while this study going run the mentioned cloud computing services. Google Colaboratory provides unlimited data storage by using Google drive, 13 GB Ram, two CPU cores and also GPU facilitation for 12 hours. More interestingly, the cloud computing services are free to use. The comparisons between the local machines and cloud computing services on customer segmentation will be discuss in the 5th chapter of results and analysis. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kw9Nd0p_QQd1"
      },
      "source": [
        "**Microsoft Azure**\n",
        "\n",
        "Microsoft Azure is a public cloud-computing platform. It facilitates us with a range of cloud services for “compute, analytics, storage, and networking”. Users can freely choose these services to develop new applications, or run existing applications, in the public cloud.Azure uses technology virtualization. “The virtualization separates the close coupling between a computer’s CPU or server and its operating system by means of an abstraction layer called a hypervisor. The hypervisor functions as a real computer or server and its CPU in a virtual machine. It enables to run multiple virtual machines at a time and each virtual machine can run any compatible operating system such as Windows or Linux”.\n",
        "\n",
        "Following are some Azure benefits:\n",
        "\n",
        "•\tApps management: Azure enable an organization to “build, deploy, and manage” the apps in cost-effective, quick and easier way. An organization can use cloud software to create and launch the website or web application. \n",
        "\n",
        "•\tFlexibility: Azure allows us to have customized functionalities .It gives an appreciable level of flexibility that give us the opportunity to have functionality as required. \n",
        "\n",
        "•\tAgility: Azure helps in fast deployment, operation, and scalability. As Azure is most up to date cloud technology, this feature helps to make infrastructure and applications agile.\n",
        "\n",
        "Storage: Azure has various data centers and delivery points. It provides the facilities including faster content delivery, optimal user experience; store any data, and fast data sharing across the virtual machines.\n",
        "\n",
        "•\tSecurity: Azure uses the spy-movie environment for protection. The data centers have “two-tier authentication, proxy card access readers, hand geometry biometric readers, global incident response team”. These tools provide effective protection against hackers.\n",
        "\n",
        "•\tAnalytics Support: Azure has built-in support to analyze data and derive insights, which help in “managed SQL services, Machine Learning, Stream and Cortina Analytics”. Thus, you can make smarter business decisions.\n",
        "\n",
        "•\tDisaster Recovery:  Staying Online all the time ensures customers/users’ trust. The disaster recovery features of Azure like “regional/global failover options; rolling reboots, and the hot/cold standby modes” gives a stronghold for against disasters.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8MRXBvCkQQXj"
      },
      "source": [
        "**Amazon Web Services**\n",
        "\n",
        "Amazon Web Services (AWS) is a cloud-based platform that use inter-connected web services for building solutions for business. Amazon Web Services was launched in 2002. However, it was officially launched in 2006, which included three services that were “Amazon S3 cloud storage, SQS, and EC2”. AWS gives us access to “servers, storage, databases, and a broad set of application services” over the Internet.  “It also provides a variety of build-in services, which can benefit businesses to build their custom cloud-based solutions. AWS is trusted by all type of firms because of the features it provides. AWS helps companies with a wide variety of workloads such as game development, data processing, warehousing, achieve, development” and many more. \n",
        "\n",
        "Following are some AWS benefits:\n",
        "\n",
        "•\tEasy to Use: AWS’s platform is simple even it can be used by neophyte. There will not be any problem for a new applicant as well as for an existing applicant. AWS Management Console and well-documented web services make it easy to use.\n",
        "\n",
        "•\tEnhanced Productivity: AWS provide support system to the cloud computing that remove the responsibilities and risks associated with housing internal I.T infrastructure. It reduces the need for I.T. support staff, and saves company’s time and money.\n",
        "\n",
        "•\tSecure and Reliable: For AWS we only have to pay for the services you use. The end-to-end approach of AWS secures and hardens your infrastructure. Amazon Web Service facilitates their subscribers essential security at a lower cost. AWS ensure data security and data privacy of that is stored in AWS data centers. AWS infrastructure helps to keep   data safe regardless of size of data. \n",
        "\n",
        "•\tFlexible and Customizable: AWS allows us to choose “programming language, operating system, database, and other assets”, so we can make fast, flexible, secure, and budget-friendly customized solution for a team. The simplicity and user-friendly nature of AWS make it attractive for some businesses.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "HIKRmsd8QQRU"
      },
      "source": [
        "**Google Colaboratory**\n",
        "\n",
        "Google Colaboratory provides unlimited data storage by using Google drive, 13 GB Ram, two CPU cores and GPU facilitation for 12 hours. More interestingly, the cloud computing services are free to use.Google Colab is one of the free cloud service that is based on Jupyter Notebook and can be used for machine learning research purpose. It provides a runtime fully configured for machine learning and free-of-charge access to a robust GPU. The comparisons between the local machines and cloud computing services on customer segmentation will be discuss in the 5th chapter of results and analysis. The dataset and the code file have been uploaded on colab notebooks folder of Google drive for using the cloud computing services of Google Colaboratory. By using this cloud and a local machine service, the following implementation was performed. While experimentation, we have observed that experiments on Google Collaboratory service took less time as compared with local machine. It has also been observed in another study  that Google Colaboratory resources preformed equivalent to a dedicated hardware. To use Google colab we will the following as:\n",
        "\n",
        "•\twe’ll need to install all specific libraries which do not come with standard python (and we’ll need to repeat this with every session).\n",
        "\n",
        "•\tGoogle Storage is used with the current session, so if we have downloaded a file and want to use it later, we’d better save it before closing the session.\n",
        "\n",
        "•\tIt can be difficult (and potentially costly) to work with bigger datasets as we have to download and store them in Google drive (only 15GB is free in Google Drive).\n",
        "during the research Google Colaboratory notebooks will be used for the analysis of the dataset, preprocessing of the dataset, clustering of the dataset and finally the classification of customers segmentation with the use of different machine learning algorithms.\n",
        "\n",
        "Beside the services and performance, we have also observed that sharing the notebook and code with output was straightforward. We have also noticed that the Google Colaboratory service had the limitation of transferring data over a drive. During experimentations, the Random Forest classifiers outperformed the other models.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e8MOB4AWQQLM"
      },
      "source": [
        "# **Testing Setup and Implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DXYqb1K3QQFF"
      },
      "source": [
        "First of all we will apply preprocessing of dataset and after that clustering is applied to that preprocessed dataset. We have also applied different classification algorithms such as Support Vector Machine, AdaBoost, Random Forest, Decision Tree, K-Nearest Neighbors, Gradient Boosting Classifier, and Logistic Regression. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QYLmcpPrQP82"
      },
      "source": [
        "**Testing Setup**\n",
        "\n",
        "The dataset and the code file have been uploaded on colab notebooks folder of Google drive for using the cloud computing services of Google Colaboratory. By using this cloud and a local machine service, the following implementation steps will be performed. In this implementation clusters of data were created based on the buying behavior of the customers in dataset. The dataset was tagged based on these clusters. Now, the tagged dataset used to implement the different classification algorithms (Support Vector Machine, AdaBoost, Random Forest, Decision Tree, K-Nearest Neighbors, Gradient Boosting Classifier, and Logistic Regression). We have used 2 months data of clients as test set and rest of the other is as training set.   \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IMGCx6z1QP3F"
      },
      "source": [
        "**Implementation**\n",
        "\n",
        "We run the following experimentation of Google Colaboratory notebook.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "86cyzM_1QPv9"
      },
      "source": [
        "**Data Preprocessing**\n",
        "\n",
        "Our Experimentations starts with the preprocessing and analysis of the data which includes Data Preprocessing and exploring the content of variables. The following subsections discussed the detailed steps involved in data preprocessing.  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6ggMykZ9QPp5"
      },
      "source": [
        "**Data Preparation**\n",
        "\n",
        "The Description and CustomerID in dataset contain some null values. Total Null values for Description is 1454 and for CustomerID is 135080. Percentages for both of them are 0.26% and 24.9 % respectively for Description and CustomerID. During the preparation of the dataset we have deleted all records that contained Null values. Null value removal when we retrieve the data with Null values, we got 0 number of records. \n",
        "After this, we check for the duplicate records. After querying the duplicate record from dataset we find that, there exist total 5225 duplicate records. We also removed all these duplicate records from dataset.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7boUyWvVQPiO"
      },
      "source": [
        "**Exploring the content of variables**\n",
        "\n",
        "While exploring and grouping the country from dataset we have find that there are total 37 countries exists in the dataset. Red color indicates that, we got 15k or above customers from that particular country. Similarly while exploring this graph we have found that we have got 19.8 K orders from United Kingdom.   \n",
        "We have also explored the product-wise transactions of customers. The data concern 4372 users and that they bought 3684 different products. The total number of transactions carried out is of the order of ∼22000. \n",
        "\n",
        "We have also found the number of products purchased in every transaction. We can see that Customer ID 12346 ordered a product against invoice no of 541431. \n",
        "\n",
        "We noted that the number of cancellations was quite large. Total 16.47% transactions found in database contained status canceled with 3654 out of 22190 transactions. \n",
        "\n",
        "We noted that when an order is canceled, we have another transaction as well mostly identical except for the Quantity and Invoice Date. We located the entries that indicate a negative quantity and check if there is systematically an order indicating the same quantity (but positive), with the same description r(CustomerID, Description and UnitPrice). There were also some transactions of cancelation of an order whose counterpart was not available in dataset.\n",
        "\n",
        "For the cancellations without counterparts, a few of them are probably due to the fact that the buy orders were performed before December 2010 (the point of entry of the database). We census of the cancel orders and check for the existence of counterparts.\n",
        "\n",
        "We checked for two cases one is a cancel order exists without counterpart and secondly when there's at least one counterpart with the exact same quantity, indicated as doubtfull_entry and entry_to_remove. We got total 7521 records for entry_to_remove and 1226 for doubtfull_entry. \n",
        "\n",
        "The doubtfull_entry list corresponds to the entries indicating a cancellation but for which there is no command beforehand. In practice, we delete all of these entries, whose count was ∼1.4% and 0.2% respectively.\n",
        "It was also observed that some values of the StockCode variable indicate a particular transaction (i.e. D for Discount). \n",
        "\n",
        "**Insights on product category**\n",
        "\n",
        "Products are uniquely identified through the StockCode in dataset. A short description of the products was added in the Description variable while data entry of the orders. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LxyxLOyTRdpw"
      },
      "source": [
        "**Product Description**\n",
        "\n",
        "We extracted information from the Description variable that will prove useful. We extract the names (proper, common) appearing in the products description for each name. We extract the root of the word and aggregate the set of names associated with this particular root. We count the number of times each root appears in the description. When several words are listed for the same root, then we consider that the keyword associated with this root is the shortest name (this systematically selects the singular when there are singular/plural variants). This function returns the three variables, 1) keywords: the list of extracted keywords, 2) keywords_roots: a dictionary where the keys are the keywords roots and the values are the lists of words associated with those roots, and 3) count_keywords: dictionary listing the number of times every word is used. We converted the count_keywords dictionary into a list, to sort the keywords according to their occurrences using this frequency sequence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XrakrelaReKJ"
      },
      "source": [
        "**Clustering**\n",
        "\n",
        "The list that was obtained contains “more than 1400 keywords and the most frequent ones appear in more than 200 products. However, while examining the content of the list we noted that some names are useless. Others are do not carry information, like colors. Therefore, we discard these words from the analysis that follows” and also, we decide to consider only the words that appear more than 13 times. \n",
        "In this section, we will group the products into different classes. In the case of matrices with binary encoding, the most suitable metric for the calculation of distances is the Hamming's metric. Note that the kmeans method of sklearn uses a Euclidean distance that can be used, but it is not to the best choice in the case of categorical variables. However, in order to use the Hamming's metric, we need to use the kmodes package which is not available on the current platform. Hence, we used the kmeans method even if this is not the best choice.\n",
        "In order to define (approximately). We use the silhouette score. In practice, the scores obtained above can be considered equivalent since, depending on the run, scores of 0.1±0.050.1±0.05 will be obtained for all clusters with n_clusters >> 3 (we obtain slightly lower scores for the first cluster). On the other hand, we found that beyond 5 clusters, some clusters contained very few elements. We therefore choose to separate the dataset into 5 clusters. In order to ensure a good classification at every run of the notebook, we iterate until we obtain the best possible silhouette score, which is, in the present case, around 0.15.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "dSswyyzIRewq"
      },
      "source": [
        "**Characterizing the content of the clusters**\n",
        "\n",
        "We checked the number of elements in every class. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5mnlbyYxReqZ"
      },
      "source": [
        "**a / Silhouette intra-cluster score**\n",
        "\n",
        "In order to have an insight on the quality of the classification, we can represent the silhouette scores of each element of the different clusters. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ftVxYVEtRekd"
      },
      "source": [
        "**b/ Word Cloud**\n",
        "\n",
        "In order to obtain a global view of their contents, we determined which keywords are the most frequent in each of them and we output the result as wordclouds.\n",
        "From this representation, we can see that for example, one of the clusters contains objects that could be associated with gifts (keywords: Christmas, packaging, card). Another cluster would rather contain luxury items and jewelry (keywords: necklace, bracelet, lace, and silver). Nevertheless, it can also be observed that many words appear in various clusters and it is therefore difficult to clearly distinguish them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "7vE_w_WfReeO"
      },
      "source": [
        "**c / Principal Component Analysis**\n",
        "\n",
        "In order to ensure that these clusters are truly distinct, we looked at their composition. Given the large number of variables of the initial matrix, we first performed a PCA and then check for the amount of variance explained by each component.\n",
        "\n",
        "We see that the number of components required to explain the data is extremely important. We need more than 100 components to explain 90% of the variance of the data. In practice, we decide to keep only a limited number of components since this decomposition is only performed to visualize the data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bVRZSv4hReYJ"
      },
      "source": [
        "**Customer categories**\n",
        "\n",
        "**Formatting data**\n",
        "\n",
        "In the previous section, the different products were grouped in five clusters. In order to prepare the rest of the analysis, a first step consists in introducing this information into a variable. To do this, we created the categorical variable categ_product where we indicate the cluster of each product.\n",
        "\n",
        "**Grouping products**\n",
        "\n",
        "In a second step, We decided to create the categ_N variables (with N∈[0:4]N∈[0:4]) that contains the amount spent in each product category.\n",
        "\n",
        "**Separation of data over time**\n",
        "\n",
        "The variable basket_price contains information for a period of 12 months. Later, one of the objectives will be to develop a model capable of characterizing and anticipating the habits of the customers visiting the site and this, from their first visit. In order to be able to test the model in a realistic way, we split the data set by retaining the first 10 months to develop the model and the following two months to test it.\n",
        "\n",
        "**Consumer Order Combinations**\n",
        "\n",
        "In a second step, we grouped together the different entries that correspond to the same user. We thus determine the number of purchases made by the user, as well as the minimum, maximum, average amounts and the total amount spent during all the visits.\n",
        "\n",
        "\n",
        "Finally, We defined two additional variables that give the number of days elapsed since the first purchase (First Purchase ) and the number of days since the last purchase ( Last Purchase ).\n",
        "\n",
        "**Data encoding**\n",
        "\n",
        "We created clusters of customers. In practice, before creating these clusters, it is interesting to define a base of smaller dimension allowing describing the scaled_matrix matrix. In this case, we used this base in order to create a representation of the different clusters and thus verify the quality of the separation of the different groups. We therefore perform a PCA beforehand and represent the amount of variance explained by each of the components.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H5lDqrgySYZu"
      },
      "source": [
        "**Creation of customer categories**\n",
        "\n",
        "At this point, we define clusters of clients from the standardized matrix that was defined earlier and using the k_mean algorithm from scikit_learn. We chooses the number of clusters based on the silhouette score and we found that the best score is obtained with 11 clusters. At first, we look at the number of customers in each cluster.\n",
        "\n",
        "**a / Report via the PCA**\n",
        "\n",
        "At first, we used the result of the PCA in order to create a representation of the various clusters. From this representation, it can be seen, for example that the first principal component allow to separate the tiniest clusters from the rest. More generally, we see that there is always a representation in which two clusters will appear to be distinct.\n",
        "\n",
        "**b/ Score de silhouette intra-cluster**\n",
        "\n",
        "As with product categories, another way to look at the quality of the separation is to look at silhouette scores within different clusters.\n",
        "\n",
        "**c/ Customers morphotype**\n",
        "\n",
        "At this stage, we have verified that the different clusters are indeed disjoint (at least, in a global way). It remains to understand the habits of the customers in each cluster. To do so, we start by adding to the selected_customers a variable that defines the cluster to which each client belongs.\n",
        "Then, we average the contents of this vari3able by first selecting the different groups of clients. This gives access to, for example, the average baskets price, the number of visit or the total sums spent by the clients of the different clusters. We also determine the number of clients in each group (variable size).\n",
        "Finally, we re-organize the content of the variables by ordering the different clusters: first, in relation to the amount wpsent in each product category and then, according to the total amount spent.\n",
        "\n",
        "**d / Customers morphology**\n",
        "\n",
        "Finally, we created a representation of the different morphotypes. This allows having a global view of the content of each cluster. It can be seen, for example, that the first 5 clusters correspond to a strong preponderance of purchases in a particular category of products. Other clusters will differ from basket averages ( mean ), the total sum spent by the clients ( sum ) or the total number of visits made ( count ).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MH9L-FMNSYTo"
      },
      "source": [
        "**Classification of customers**\n",
        "In this part, the objective was to adjust a classifier that will classify consumers in the different client categories that were established in the previous section. The objective is to make this classification possible at the first visit. To fulfill this objective, we tested several classifiers implemented in scikit-learn. First, in order to simplify their use, then defined a class that allows to interface several of the functionalities common to these different classifiers.\n",
        "\n",
        "Since the goal is to define the class to which a client belongs and this, as soon as its first visit, we only keep the variables that describe the content of the basket, and do not take into account the variables related to the frequency of visits or variations of the basket price over time. Finally, we split the dataset in train and test sets.\n",
        "Once this instance is created, we adjusted the classifier to the training data, then we can tested the quality of the prediction with respect to the test data.\n",
        "\n",
        "**Support Vector Machine Classifier (SVC)**\n",
        "\n",
        "The first classifier we use is the SVC classifier. In order to use it, we create an instance of the Class_Fit class and then call grid_search(). When calling this method, we provide as parameters. \n",
        "\n",
        "•\tthe hyper parameters for which will seek an optimal value\n",
        "\n",
        "•\tthe number of folds to be used for cross-validation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3CkjxK1vSYN8"
      },
      "source": [
        "**Confusion matrix**\n",
        "\n",
        "The accuracy of the results seems to be correct. Nevertheless, let us remember that when the different classes were defined, there was an imbalance in size between the classes obtained. In particular, one class contains around 40% of the clients. It is therefore interesting to look at how the predictions and real values compare to the breasts of the different classes. This is the subject of the confusion matrices and to represent them.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GI-TF8rYSYHs"
      },
      "source": [
        "**Learning curve**\n",
        "\n",
        "On the curve, as can see that the train and cross-validation curves converge towards the same limit when the sample size increases. This is typical of modeling with low variance and proves that the model does not suffer from overfitting. Also, as can see that the accuracy of the training curve is correct which is synonymous of a low bias. Hence the model does not under fit the data. We tested the quality of the prediction with respect to the test data and got precision of   80.75 %.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "M3qaeu9NSYBp"
      },
      "source": [
        "**Logistic Regression**\n",
        "\n",
        "Now considering the logistic regression classifier. As before, created an instance of the Class_Fit class, adjust the model on the training data and see how the predictions compare to the real values. We tested the quality of the prediction with respect to the test data and got precision of   86.29 %.\n",
        "\n",
        "**k-Nearest Neighbors**\n",
        "\n",
        "We tested the quality of the prediction with respect to the test data and got precision of   79.78 %.\n",
        "\n",
        "**Decision Tree**\n",
        "\n",
        "We tested the quality of the prediction with respect to the test data and got precision of   83.24 %.\n",
        "\n",
        "**Random Forest**\n",
        "\n",
        "We tested the quality of the prediction with respect to the test data and got precision of   89.61 %.\n",
        "\n",
        "**AdaBoost Classifier**\n",
        "\n",
        "We tested the quality of the prediction with respect to the test data and got precision of   54.57 %.\n",
        "\n",
        "**Gradient Boosting Classifier**\n",
        "\n",
        "We tested the quality of the prediction with respect to the test data and got precision of   89.47 %.\n",
        "\n",
        "**Voting** \n",
        "\n",
        "Finally, we created a prediction for this model. While defining the voting classifier, we only used a sub-sample of the whole set of classifiers defined above and only retained the Random Forest, the k-Nearest Neighbors and the Gradient Boosting classifiers. We got Precision equal to 90.03 %.\n",
        "\n",
        "**Testing Predictions**\n",
        "\n",
        "In the previous section, a few classifiers were trained in order to categorize customers. Until that point, the whole analysis was based on the data of the first 10 months. Now, we tested the model on the last two months of the dataset.\n",
        "We prepared the test data by defining the category to which the customers belong. However, this definition used data obtained over a period of 2 months (via the variables count , min , max and sum ). The classifier defined and used a more restricted set of variables that will be defined from the first purchase of a client.\n",
        "The question arose of using the available data over a period of two months and using this data to define the category to which the customers belong. Then, the classifier was tested by comparing its predictions with these categories. In order to define the category to which the clients belong, we recalled the instance of the k means method. The predicted method of this instance calculated the distance of the consumers from the centroids of the 11 client classes and the smallest distance defined the belonging to the different categories.\n",
        "Finally, in order to prepare the execution of the classifier, it was sufficient to select the variables on which it acts. It remains only to examine the predictions of the different classifiers.\n",
        "Finally, as anticipated that, it is possible to improve the quality of the classifier by combining their respective predictions. At this level, we chose to mix Random Forest, Gradient Boosting and k-Nearest Neighbors predictions because this leads to a slight improvement in predictions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "h4eenJaVSX74"
      },
      "source": [
        "# **Results and Analysis**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MODiQAMlSX1y"
      },
      "source": [
        "In previous chapter, we have discussed the techniques that we used in this research while in this chapter we will discuss the results of these methods. For the experimentation, we used Cloud services for customer segmentation for the first time that has not been explored previously. We evaluated results of different algorithms such as Support Vector Machine, AdaBoost, Random Forest, Decision Tree, K-Nearest Neighbors, Gradient Boosting Classifier, and Logistic Regression while using the cloud environment that has many benefits. These benefits includes the Cost Saving Factors, security, Flexibility, Mobility, Insight, Increased Collaboration, Quality Control, Disaster Recovery, Loss Prevention, Automatic Software Updates, Competitive Edge and Sustainability."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EZctbzP5SXoY"
      },
      "source": [
        "**Evaluation by cloud computing services**\n",
        "\n",
        "We have implemented the AdaBoot algorithm using both cloud. Confusion matrix using AdaBoost classifier is given where we can see that 32 examples of class 0 predicted as 6. Similarly, 60, 85, 302, 44,   27, and 60 examples of class 1, 2, 6, 7, 8, 9 respectively predicted as class 6. While experimenting over local machine and cloud services, we tested the quality of the prediction with respect to the test data and got Precision, Recall, F1 measure and accuracy score of 0.32, 0.55, 0.40 and 0.55 respectively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_OJ0OQXuSXhm"
      },
      "source": [
        "We have also implemented the Decision Tree algorithm using both cloud and local machine services. Confusion matrix using Decision Tree algorithm is given where we can see that 60 examples of class 1 is also predicted as 1. Maximum examples that are predicted correctly, that were for class 6, where 261 examples predicted correctly. While experimenting over local machine and cloud services, we tested the quality of the prediction with respect to the test data and got Precision, Recall, F1 measure and accuracy score of 0.84, 0.84, 0.84 and 0.84 respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gvaxTY8oSXbL"
      },
      "source": [
        "We have also implemented the Gradient Boost classifier using both cloud and local machine services. Confusion matrix using Gradient Boost classifier is given where we can see that 58 examples of class 1 is also predicted as 1. Maximum examples that are predicted correctly, that were for class 6, where 291 examples predicted correctly. While experimenting over local machine and cloud services, we tested the quality of the prediction with respect to the test data and got Precision, Recall, F1 measure and accuracy score of 0.85, 0.89, 0.86 and 0.89 respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "tXll_K-3SXTg"
      },
      "source": [
        "We have also implemented the Logistic Regression classifier using both cloud and local machine services. Confusion matrix using Logistic Regression classifier is given where we can see that 60 examples of class 1 is also predicted as 1. Maximum examples that are predicted correctly, that were for class 6, where 300 examples predicted correctly. While experimenting over local machine and cloud services, we tested the quality of the prediction with respect to the test data and got Precision, Recall, F1 measure and accuracy score of 0.82, 0.87, 0.84 and 0.87 respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nAFm7AhxSXNQ"
      },
      "source": [
        "We have also implemented the KNN classifier using both cloud and local machine services. Confusion matrix using KNN classifier is given where we can see that 44 examples of class 1 is also predicted as 1. Maximum examples that are predicted correctly, that were for class 6, where 279 examples predicted correctly. While experimenting over local machine and cloud services, we tested the quality of the prediction with respect to the test data and got Precision, Recall, F1 measure and accuracy score of 0.79, 0.81, 0.79 and 0.81 respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "3jV6g31NSXFy"
      },
      "source": [
        "We have also implemented the SVM classifier using both cloud and local machine services. Confusion matrix using SVM classifier is given where we can see that 63 examples of class 1 is also predicted as 1. Maximum examples that are predicted correctly, that were for class 6, where 252 examples predicted correctly. While experimenting over local machine and cloud services, we tested the quality of the prediction with respect to the test data and got Precision, Recall, F1 measure and accuracy score of 0.83, 0.86, 0.84 and 0.86 respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0DgRRYpsT6R0"
      },
      "source": [
        "We have also implemented the Gradient Boosting classifier using both cloud and local machine services. Confusion matrix using Gradient Boosting classifier is given where we can see that 58 examples of class 1 is also predicted as 1. Maximum examples that are predicted correctly, that were for class 6, where 291 examples predicted correctly. While experimenting over local machine and cloud services, we tested the quality of the prediction with respect to the test data and got Precision, Recall, F1 measure and accuracy score of 0.85, 0.89, 0.86 and 0.89 respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jhnKkZ2CT6Le"
      },
      "source": [
        "We have also implemented the Random Forest using both cloud and local machine services. Confusion matrix using Gradient Boosting classifier is given where we can see that 61 examples of class 1 is also predicted as 1. Maximum examples that are predicted correctly, that were for class 6, where 289 examples predicted correctly. While experimenting over local machine and cloud services, we tested the quality of the prediction with respect to the test data and got Precision, Recall, F1 measure and accuracy score of 0.86, 0.90, 0.87 and 0.90 respectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hRVR87eMT6Fs"
      },
      "source": [
        "We have observed that experiments on Google Colaboratory service took less time as compared with local machine. It has also been observed in another study that Google Colaboratory resources preformed equivalent to a dedicated hardware. Beside the services and performance, we have also observed that sharing the notebook and code with output was straightforward. We have also noticed that the Google Colaboratory service had the limitation of transferring data over a drive. During experimentations, the Random Forest classifiers outperformed the other models. This is due to the reason that this model works well with a mixture of numerical and floating number features. It also performs well when features are based on the various scales. Random Forest classifier run fast as compare to other models and its accuracy is also high as compare to other models.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "nU4AkgfcT5_a"
      },
      "source": [
        "# **Conclusion** "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "b2dxf5-nT55Y"
      },
      "source": [
        "**Summary**\n",
        "\n",
        "Cloud service is a type of service that covers the extensive range of resources that are provided as service to the customers, broadly known as cloud.  Cloud services provide self-provisioning and elasticity that a customer can activate and shut down services on-demand. Cloud services are normally better as compare to local machine as they provide more computational power with minimum cost. Google Colaboratory service is free of cost service for Jupyter notebook, which can be used for any type of machine learning task. Customer segmentation is a method of segmenting the customers on the bases of their buying behaviors. This was done by applying clustering and different machine learning algorithms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "crIP_jIeT5xw"
      },
      "source": [
        "The work described in this study is based on a database providing details on purchases made on an E-commerce platform over a period of one year. Each entry in the dataset describes the purchase of a product, by a particular customer and at a given date. In total, approximately 4000 clients appear in the database. Given the available information, we decided to develop a classifier that allows to anticipate the type of purchase that a customer will make, as well as the number of visits that he will make during a year, and this from its first visit to the E-commerce site."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L1wwpbmDULgf"
      },
      "source": [
        "The first stage of this work consisted in describing the different products sold by the site, which was the subject of a first classification. There, we grouped the different products into 5 main categories of goods. In a second step, we performed a classification of the customers by analyzing their consumption habits over a period of 10 months. We have classified clients into 11 major categories based on the type of products they usually buy, the number of visits they make and the amount they spent during the 10 months. Once these categories were established, we finally trained several classifiers over local machine and cloud service, whose objective was to be able to classify consumers in one of these 11 categories and this from their first purchase. We found that 75% of clients are awarded the right classes. The performance of the classifier therefore seems correct given the potential shortcomings of the current model. In particular, a bias that has not been dealt with concerns the seasonality of purchases and the fact that purchasing habits will potentially depend on the time of year (for example, Christmas). In the practice, this seasonal effect may cause the categories defined over a 10-month period to be quite different from those extrapolated from the last two months. In order to correct such bias, it would be beneficial to have data that would cover a longer period of time."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "77841518-29e7-4001-9aeb-6267400da121",
        "_uuid": "baf39e29f412685d42adbf39a6030e5f00255d0d",
        "colab_type": "text",
        "id": "9l-ITPu-ww2p"
      },
      "source": [
        "# Customer segmentation\n",
        "___\n",
        "**1. Data Preparation**\n",
        "\n",
        "**2. Exploring the content of variables**\n",
        "\n",
        "   - 2.1 Countries\n",
        "   - 2.2 Customers and products\n",
        "       * 2.2.1 Cancelling orders\n",
        "       * 2.2.2 StockCode\n",
        "       * 2.2.3 Basket price\n",
        "\n",
        "**3. Insight on product categories**\n",
        "\n",
        "   - 3.1 Product description \n",
        "   - 3.2 Defining product categories \n",
        "       * 3.2.1 Data encoding\n",
        "       * 3.2.2 Clusters of products\n",
        "       * 3.2.3 Characterizing the content of clusters\n",
        "   \n",
        "**4. Customer categories**\n",
        "\n",
        "   - 4.1 Formating data\n",
        "       * 4.1.1 Grouping products \n",
        "       * 4.1.2 Time spliting of the dataset\n",
        "       * 4.1.3 Grouping orders \n",
        "   - 4.2 Creating customer categories\n",
        "       * 4.2.1 Data enconding\n",
        "       * 4.2.2 Creating categories\n",
        "\n",
        "**5. Classifying customers**\n",
        "\n",
        "   - 5.1 Support Vector Machine Classifier (SVC)\n",
        "   - 5.2 Logistic regression \n",
        "   - 5.3 k-Nearest Neighbors\n",
        "   - 5.4 Decision Tree\n",
        "   - 5.5 Random Forest\n",
        "   - 5.6 AdaBoost\n",
        "   - 5.7 Gradient Boosting Classifier\n",
        "   - 5.8 Let's vote !\n",
        "   \n",
        "**6. Testing the predictions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "151b89ad-6783-483e-8cf9-081f23052f7f",
        "_uuid": "b2c01e22363fb1afa2ea2b080659cbfbe0416465",
        "colab_type": "text",
        "id": "PqeP_FpTww2r"
      },
      "source": [
        "## 1. Data preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "705714b1-870b-4f34-b5bf-cd027dafaefe",
        "_kg_hide-input": true,
        "_uuid": "bb40a7b23734d82876d812fab6daecd83a46368c",
        "colab_type": "code",
        "id": "pjPPKJzaww2s",
        "outputId": "503ce984-34bd-4ae6-d444-a56e0b840af7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as mpl\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import datetime, nltk, warnings\n",
        "import matplotlib.cm as cm\n",
        "import itertools\n",
        "from pathlib import Path\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn import preprocessing, model_selection, metrics, feature_selection\n",
        "from sklearn.model_selection import GridSearchCV, learning_curve\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import neighbors, linear_model, svm, tree, ensemble\n",
        "from wordcloud import WordCloud, STOPWORDS\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.decomposition import PCA\n",
        "from IPython.display import display, HTML\n",
        "import plotly.graph_objs as go\n",
        "from plotly.offline import init_notebook_mode,iplot\n",
        "init_notebook_mode(connected=True)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "plt.rcParams[\"patch.force_edgecolor\"] = True\n",
        "plt.style.use('fivethirtyeight')\n",
        "mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "        <script type=\"text/javascript\">\n",
              "        window.PlotlyConfig = {MathJaxConfig: 'local'};\n",
              "        if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}\n",
              "        if (typeof require !== 'undefined') {\n",
              "        require.undef(\"plotly\");\n",
              "        requirejs.config({\n",
              "            paths: {\n",
              "                'plotly': ['https://cdn.plot.ly/plotly-latest.min']\n",
              "            }\n",
              "        });\n",
              "        require(['plotly'], function(Plotly) {\n",
              "            window._Plotly = Plotly;\n",
              "        });\n",
              "        }\n",
              "        </script>\n",
              "        "
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MB6o1k881d9o",
        "outputId": "55eb6cbf-1a05-4f98-83d2-15949908352b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 276
        }
      },
      "source": [
        "!pip install pydrive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pydrive in /usr/local/lib/python3.6/dist-packages (1.3.1)\n",
            "Requirement already satisfied: google-api-python-client>=1.2 in /usr/local/lib/python3.6/dist-packages (from pydrive) (1.7.11)\n",
            "Requirement already satisfied: oauth2client>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (4.1.3)\n",
            "Requirement already satisfied: PyYAML>=3.0 in /usr/local/lib/python3.6/dist-packages (from pydrive) (3.13)\n",
            "Requirement already satisfied: google-auth>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.4.2)\n",
            "Requirement already satisfied: httplib2<1dev,>=0.9.2 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.11.3)\n",
            "Requirement already satisfied: uritemplate<4dev,>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (3.0.0)\n",
            "Requirement already satisfied: google-auth-httplib2>=0.0.3 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (0.0.3)\n",
            "Requirement already satisfied: six<2dev,>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from google-api-python-client>=1.2->pydrive) (1.12.0)\n",
            "Requirement already satisfied: rsa>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.7 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.4.7)\n",
            "Requirement already satisfied: pyasn1-modules>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from oauth2client>=4.0.0->pydrive) (0.2.6)\n",
            "Requirement already satisfied: cachetools>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.4.1->google-api-python-client>=1.2->pydrive) (3.1.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LFRQhTuF34mp",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "#2. Get the file\n",
        "downloaded = drive.CreateFile({'id':'1VPbWORCZ7p1BnxrovGsqJTUnAwLKMQTz'}) # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('data.csv')  \n",
        "\n",
        "#3. Read file as panda dataframe\n",
        "import pandas as pd\n",
        "df_initial = pd.read_csv('data.csv',encoding=\"ISO-8859-1\",\n",
        "                         dtype={'CustomerID': str,'InvoiceID': str})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "1063f9e0-e494-4873-939f-8aa5ca40cc89",
        "_kg_hide-input": true,
        "_uuid": "227fc0cb1d5216d52e057e1d2d7debd0e29abe46",
        "colab_type": "code",
        "id": "0-9ZQr6Eww2x",
        "scrolled": false,
        "colab": {}
      },
      "source": [
        "\n",
        "print('Dataframe dimensions:', df_initial.shape)\n",
        "#______\n",
        "df_initial['InvoiceDate'] = pd.to_datetime(df_initial['InvoiceDate'])\n",
        "#____________________________________________________________\n",
        "# gives some infos on columns types and numer of null values\n",
        "tab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\n",
        "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n",
        "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()/df_initial.shape[0]*100).T.\n",
        "                         rename(index={0:'null values (%)'}))\n",
        "display(tab_info)\n",
        "#__________________\n",
        "# show first lines\n",
        "display(df_initial[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "f9de6b67-a588-43ab-8f51-b28efdee9e32",
        "_kg_hide-input": true,
        "_uuid": "9b915fa18b311e8f93ac862bd49d08d90e03ca48",
        "colab_type": "code",
        "id": "25CR3Khqww22",
        "colab": {}
      },
      "source": [
        "df_initial.dropna(axis = 0, subset = ['CustomerID'], inplace = True)\n",
        "print('Dataframe dimensions:', df_initial.shape)\n",
        "#____________________________________________________________\n",
        "# gives some infos on columns types and numer of null values\n",
        "tab_info=pd.DataFrame(df_initial.dtypes).T.rename(index={0:'column type'})\n",
        "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()).T.rename(index={0:'null values (nb)'}))\n",
        "tab_info=tab_info.append(pd.DataFrame(df_initial.isnull().sum()/df_initial.shape[0]*100).T.\n",
        "                         rename(index={0:'null values (%)'}))\n",
        "display(tab_info)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "baf1ff2e-646b-468b-b7b4-68343f388387",
        "_uuid": "6b988d1dee3deecafd54f2b3555d1f84b509d213",
        "colab_type": "code",
        "id": "PvDC18Czww25",
        "colab": {}
      },
      "source": [
        "print('Entries dublicate: {}'.format(df_initial.duplicated().sum()))\n",
        "df_initial.drop_duplicates(inplace = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ba32e71f-0e94-4d44-81cf-8409d644fc8b",
        "_uuid": "bd5c8b02bb28eaa95b66c941143794440e4ef53b",
        "colab_type": "text",
        "id": "MFwfmMKaww27"
      },
      "source": [
        "___\n",
        "## 2. Exploring the content of variables\n",
        "\n",
        "\n",
        "___\n",
        "### 2.1 Countries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "44abc17d-8858-457a-94e1-f012143fda87",
        "_kg_hide-input": true,
        "_uuid": "c4141f12a8b2c733539a75d398cadf0817ca0969",
        "colab_type": "code",
        "id": "ShamQ5PAww28",
        "colab": {}
      },
      "source": [
        "temp = df_initial[['CustomerID', 'InvoiceNo', 'Country']].groupby(['CustomerID', 'InvoiceNo', 'Country']).count()\n",
        "temp = temp.reset_index(drop = False)\n",
        "countries = temp['Country'].value_counts()\n",
        "print('Count: {}'.format(len(countries)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "993e237f-8fbc-47b6-a2f9-6418a7221f07",
        "_kg_hide-input": true,
        "_uuid": "7b84b31aa58c137c8bb05fe5a3ebcb791b13100b",
        "colab_type": "code",
        "id": "IPkwwFDEww2_",
        "colab": {}
      },
      "source": [
        "data = dict(type='choropleth',\n",
        "locations = countries.index,\n",
        "locationmode = 'country names', z = countries,\n",
        "text = countries.index, colorbar = {'title':'Order nb.'},\n",
        "colorscale=[[0, 'rgb(224,255,255)'],\n",
        "            [0.01, 'rgb(166,206,227)'], [0.02, 'rgb(31,120,180)'],\n",
        "            [0.03, 'rgb(178,223,138)'], [0.05, 'rgb(51,160,44)'],\n",
        "            [0.10, 'rgb(251,154,153)'], [0.20, 'rgb(255,255,0)'],\n",
        "            [1, 'rgb(227,26,28)']],    \n",
        "reversescale = False)\n",
        "#_______________________\n",
        "layout = dict(title='Number of orders per country',\n",
        "geo = dict(showframe = True, projection={'type':'mercator'}))\n",
        "#______________\n",
        "choromap = go.Figure(data = [data], layout = layout)\n",
        "iplot(choromap, validate=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "a0a41461-7f22-4ca8-bc97-5aba4249dde7",
        "_uuid": "7a29044f1d76c8fb064a3262f5877ab27673a836",
        "colab_type": "text",
        "id": "oPxDBifsww3D"
      },
      "source": [
        "\n",
        "### 2.2 Customers and products"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "677f103d-d199-480c-bca4-fc08f7aa2e92",
        "_kg_hide-input": true,
        "_uuid": "dc2f4b48b76615721e6718efbd31fcd3faf16bec",
        "colab_type": "code",
        "id": "aWaH3nW7ww3D",
        "colab": {}
      },
      "source": [
        "pd.DataFrame([{'products': len(df_initial['StockCode'].value_counts()),    \n",
        "               'transactions': len(df_initial['InvoiceNo'].value_counts()),\n",
        "               'customers': len(df_initial['CustomerID'].value_counts()),  \n",
        "              }], columns = ['products', 'transactions', 'customers'], index = ['quantity'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "72d6dede-4280-4afd-b61b-085ea8c73d67",
        "_kg_hide-input": true,
        "_uuid": "dd0d84bd4275a04e361b5b41924d11b7f6f2e9ff",
        "colab_type": "code",
        "id": "EfSzS6uhww3G",
        "colab": {}
      },
      "source": [
        "temp = df_initial.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate'].count()\n",
        "nb_products_per_basket = temp.rename(columns = {'InvoiceDate':'Number of products'})\n",
        "nb_products_per_basket[:10].sort_values('CustomerID')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "ac04f52f-3847-400b-abc8-2e04a4145ac8",
        "_uuid": "b4d634aa7a7a44548f5e20d4c84c9c5b16e3b5a7",
        "colab_type": "text",
        "id": "FWJVDZ5uww3I"
      },
      "source": [
        "\n",
        "#### 2.2.1 Cancelling orders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "9b0e32d8-fc9c-4301-ac18-9c1d7cc5b54f",
        "_kg_hide-input": true,
        "_uuid": "076fba25ed8a2b38fddd83ff862fa21e7f790a11",
        "colab_type": "code",
        "id": "rSsoUDabww3J",
        "colab": {}
      },
      "source": [
        "nb_products_per_basket['order_canceled'] = nb_products_per_basket['InvoiceNo'].apply(lambda x:int('C' in x))\n",
        "display(nb_products_per_basket[:5])\n",
        "#______________________________________________________________________________________________\n",
        "n1 = nb_products_per_basket['order_canceled'].sum()\n",
        "n2 = nb_products_per_basket.shape[0]\n",
        "print('Number of orders canceled: {}/{} ({:.2f}%) '.format(n1, n2, n1/n2*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "2f985d42-e0b9-4281-8f30-43c2c13955b9",
        "_kg_hide-input": true,
        "_uuid": "54f5b8a4bc832c1c396419223f43c41b9b0b27de",
        "colab_type": "code",
        "id": "4ODt2Dfqww3N",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "display(df_initial.sort_values('CustomerID')[:5])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "06e26580-014e-432d-ab59-b5ceebb816cb",
        "_kg_hide-input": false,
        "_uuid": "b16ddfdd36696a4a92ba15acd387e7eca0757f31",
        "colab_type": "code",
        "id": "NpUiye7Zww3P",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "df_check = df_initial[df_initial['Quantity'] < 0][['CustomerID','Quantity',\n",
        "                                                   'StockCode','Description','UnitPrice']]\n",
        "for index, col in  df_check.iterrows():\n",
        "    if df_initial[(df_initial['CustomerID'] == col[0]) & (df_initial['Quantity'] == -col[1]) \n",
        "                & (df_initial['Description'] == col[2])].shape[0] == 0: \n",
        "        print(df_check.loc[index])\n",
        "        print(15*'-'+'>'+' HYPOTHESIS NOT FULFILLED')\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "50c6589e-4387-4a3e-a1ea-18bee5bb1dba",
        "_kg_hide-input": true,
        "_uuid": "3b375d17a84505d71b20dbbc43124aa6597a2ef2",
        "colab_type": "code",
        "id": "TrqvYACRww3S",
        "colab": {}
      },
      "source": [
        "df_check = df_initial[(df_initial['Quantity'] < 0) & (df_initial['Description'] != 'Discount')][\n",
        "                                 ['CustomerID','Quantity','StockCode',\n",
        "                                  'Description','UnitPrice']]\n",
        "\n",
        "for index, col in  df_check.iterrows():\n",
        "    if df_initial[(df_initial['CustomerID'] == col[0]) & (df_initial['Quantity'] == -col[1]) \n",
        "                & (df_initial['Description'] == col[2])].shape[0] == 0: \n",
        "        print(index, df_check.loc[index])\n",
        "        print(15*'-'+'>'+' HYPOTHESIS NOT FULFILLED')\n",
        "        break"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "af540729-739b-45b3-858f-f1facf9f8ae6",
        "_kg_hide-input": true,
        "_uuid": "6f5c10794e09eb3d0dc83889d74b2029d6d24756",
        "colab_type": "code",
        "id": "3_r7eioGww3U",
        "colab": {}
      },
      "source": [
        "df_cleaned = df_initial.copy(deep = True)\n",
        "df_cleaned['QuantityCanceled'] = 0\n",
        "\n",
        "entry_to_remove = [] ; doubtfull_entry = []\n",
        "\n",
        "for index, col in  df_initial.iterrows():\n",
        "    if (col['Quantity'] > 0) or col['Description'] == 'Discount': continue        \n",
        "    df_test = df_initial[(df_initial['CustomerID'] == col['CustomerID']) &\n",
        "                         (df_initial['StockCode']  == col['StockCode']) & \n",
        "                         (df_initial['InvoiceDate'] < col['InvoiceDate']) & \n",
        "                         (df_initial['Quantity']   > 0)].copy()\n",
        "    #_________________________________\n",
        "    # Cancelation WITHOUT counterpart\n",
        "    if (df_test.shape[0] == 0): \n",
        "        doubtfull_entry.append(index)\n",
        "    #________________________________\n",
        "    # Cancelation WITH a counterpart\n",
        "    elif (df_test.shape[0] == 1): \n",
        "        index_order = df_test.index[0]\n",
        "        df_cleaned.loc[index_order, 'QuantityCanceled'] = -col['Quantity']\n",
        "        entry_to_remove.append(index)        \n",
        "    #______________________________________________________________\n",
        "    # Various counterparts exist in orders: we delete the last one\n",
        "    elif (df_test.shape[0] > 1): \n",
        "        df_test.sort_index(axis=0 ,ascending=False, inplace = True)        \n",
        "        for ind, val in df_test.iterrows():\n",
        "            if val['Quantity'] < -col['Quantity']: continue\n",
        "            df_cleaned.loc[ind, 'QuantityCanceled'] = -col['Quantity']\n",
        "            entry_to_remove.append(index) \n",
        "            break            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "f1d3a68d-fa59-4671-8be1-cdbb646ce13f",
        "_kg_hide-input": true,
        "_uuid": "d98a0917de35db7afe31c69c324cd32e934edd52",
        "colab_type": "code",
        "id": "I-SI8RaDww3W",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "print(\"entry_to_remove: {}\".format(len(entry_to_remove)))\n",
        "print(\"doubtfull_entry: {}\".format(len(doubtfull_entry)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "50f6c074-08cc-4c55-8285-d674d0d84b45",
        "_kg_hide-input": true,
        "_uuid": "c523275748742927f689725305e90ae6fdeb4136",
        "colab_type": "code",
        "id": "4O6Wuh3Oww3b",
        "colab": {}
      },
      "source": [
        "df_cleaned.drop(entry_to_remove, axis = 0, inplace = True)\n",
        "df_cleaned.drop(doubtfull_entry, axis = 0, inplace = True)\n",
        "remaining_entries = df_cleaned[(df_cleaned['Quantity'] < 0) & (df_cleaned['StockCode'] != 'D')]\n",
        "print(\"nb of entries to delete: {}\".format(remaining_entries.shape[0]))\n",
        "remaining_entries[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "86494812-a35a-49a8-a80d-5b2ef3b0913f",
        "_uuid": "5ded98b83a85e0fd038b1a5c5edb67d7773d41ee",
        "colab_type": "code",
        "id": "tECFQhbVww3d",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "df_cleaned[(df_cleaned['CustomerID'] == 14048) & (df_cleaned['StockCode'] == '22464')]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "840a05b8-da31-4a61-ba85-d26050ea7224",
        "_uuid": "ecf714d564214e6614fb604566b14d60e174a076",
        "colab_type": "text",
        "id": "bo3Nwn1Dww3g"
      },
      "source": [
        "\n",
        "#### 2.2.2 StockCode\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "e00212c8-5c1e-4dda-a392-cbc68c1964b1",
        "_kg_hide-input": true,
        "_uuid": "57e546917a0ea9a59a0e1dc3e0f9179c7efa66b5",
        "colab_type": "code",
        "id": "rXZ1LSAEww3h",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "list_special_codes = df_cleaned[df_cleaned['StockCode'].str.contains('^[a-zA-Z]+', regex=True)]['StockCode'].unique()\n",
        "list_special_codes"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "0a225335-7d6e-4c5b-a874-801e3f329f10",
        "_kg_hide-input": true,
        "_uuid": "ce078be30fea360c161b449a8cb666d98808e936",
        "colab_type": "code",
        "id": "X8iIR9J0ww3j",
        "colab": {}
      },
      "source": [
        "for code in list_special_codes:\n",
        "    print(\"{:<15} -> {:<30}\".format(code, df_cleaned[df_cleaned['StockCode'] == code]['Description'].unique()[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bc5a1aa1-9861-4f24-a40d-8a989412c9b9",
        "_uuid": "d4cf73dade342afadb96645a654171064c0426ba",
        "colab_type": "text",
        "id": "93Rwj1TSww3o"
      },
      "source": [
        "\n",
        "#### 2.2.3 Basket Price"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "3cbf20c0-0a44-49dc-96c3-ffd5455ddf0b",
        "_kg_hide-input": true,
        "_uuid": "5f070241e41d989ed3de0769d9f35f330d086415",
        "colab_type": "code",
        "id": "PZto-Ztyww3r",
        "colab": {}
      },
      "source": [
        "df_cleaned['TotalPrice'] = df_cleaned['UnitPrice'] * (df_cleaned['Quantity'] - df_cleaned['QuantityCanceled'])\n",
        "df_cleaned.sort_values('CustomerID')[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "5e4530b2-addf-4dc7-9ca8-065c26b73023",
        "_kg_hide-input": true,
        "_uuid": "653fd7be2e985cf4578af4306f40948926fb60b3",
        "colab_type": "code",
        "id": "FPOnPx2Eww30",
        "colab": {}
      },
      "source": [
        "#___________________________________________\n",
        "# sum of purchases / user & order\n",
        "temp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['TotalPrice'].sum()\n",
        "basket_price = temp.rename(columns = {'TotalPrice':'Basket Price'})\n",
        "#_____________________\n",
        "# on the order\n",
        "df_cleaned['InvoiceDate_int'] = df_cleaned['InvoiceDate'].astype('int64')\n",
        "temp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate_int'].mean()\n",
        "df_cleaned.drop('InvoiceDate_int', axis = 1, inplace = True)\n",
        "basket_price.loc[:, 'InvoiceDate'] = pd.to_datetime(temp['InvoiceDate_int'])\n",
        "#______________________________________\n",
        "# selection of significant entries:\n",
        "basket_price = basket_price[basket_price['Basket Price'] > 0]\n",
        "basket_price.sort_values('CustomerID')[:6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "25f72313-bc56-4a10-99b1-243f147b1756",
        "_kg_hide-input": true,
        "_uuid": "b1b30be7aa80d7a5287e6fd783b5d8cdbff4032d",
        "colab_type": "code",
        "id": "z0rALtxBww33",
        "colab": {}
      },
      "source": [
        "\n",
        "price_range = [0, 50, 100, 200, 500, 1000, 5000, 50000]\n",
        "count_price = []\n",
        "for i, price in enumerate(price_range):\n",
        "    if i == 0: continue\n",
        "    val = basket_price[(basket_price['Basket Price'] < price) &\n",
        "                       (basket_price['Basket Price'] > price_range[i-1])]['Basket Price'].count()\n",
        "    count_price.append(val)\n",
        "#____________________________________________\n",
        "#  Representation of the number of purchases / amount       \n",
        "plt.rc('font', weight='bold')\n",
        "f, ax = plt.subplots(figsize=(11, 6))\n",
        "colors = ['yellowgreen', 'gold', 'wheat', 'c', 'violet', 'royalblue','firebrick']\n",
        "labels = [ '{}<.<{}'.format(price_range[i-1], s) for i,s in enumerate(price_range) if i != 0]\n",
        "sizes  = count_price\n",
        "explode = [0.0 if sizes[i] < 100 else 0.0 for i in range(len(sizes))]\n",
        "ax.pie(sizes, explode = explode, labels=labels, colors = colors,\n",
        "       autopct = lambda x:'{:1.0f}%'.format(x) if x > 1 else '',\n",
        "       shadow = False, startangle=0)\n",
        "ax.axis('equal')\n",
        "f.text(0.5, 1.01, \"Distribution of order amounts\", ha='center', fontsize = 18);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "81a3ca3c-32f5-4f57-aa54-182d779a5cba",
        "_uuid": "4864a5c03f9a66bff72599c2ff348157e53c4d7c",
        "colab_type": "text",
        "id": "2OK8TP9Sww36"
      },
      "source": [
        "\n",
        "## 3. Insight on product categories\n",
        "\n",
        "\n",
        "### 3.1 Products Description\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "62aada7f-1d61-493e-a044-08fcc7bdfb81",
        "_kg_hide-input": true,
        "_uuid": "4ae364672f6cede623fd0e032e34d967e4f32ee1",
        "colab_type": "code",
        "id": "UahCk0hhww38",
        "colab": {}
      },
      "source": [
        "is_noun = lambda pos: pos[:2] == 'NN'\n",
        "\n",
        "def keywords_inventory(dataframe, colonne = 'Description'):\n",
        "    stemmer = nltk.stem.SnowballStemmer(\"english\")\n",
        "    keywords_roots  = dict()  # collect the words / root\n",
        "    keywords_select = dict()  # association: root <-> keyword\n",
        "    category_keys   = []\n",
        "    count_keywords  = dict()\n",
        "    icount = 0\n",
        "    for s in dataframe[colonne]:\n",
        "        if pd.isnull(s): continue\n",
        "        lines = s.lower()\n",
        "        tokenized = nltk.word_tokenize(lines)\n",
        "        nouns = [word for (word, pos) in nltk.pos_tag(tokenized) if is_noun(pos)] \n",
        "        \n",
        "        for t in nouns:\n",
        "            t = t.lower() ; racine = stemmer.stem(t)\n",
        "            if racine in keywords_roots:                \n",
        "                keywords_roots[racine].add(t)\n",
        "                count_keywords[racine] += 1                \n",
        "            else:\n",
        "                keywords_roots[racine] = {t}\n",
        "                count_keywords[racine] = 1\n",
        "    \n",
        "    for s in keywords_roots.keys():\n",
        "        if len(keywords_roots[s]) > 1:  \n",
        "            min_length = 1000\n",
        "            for k in keywords_roots[s]:\n",
        "                if len(k) < min_length:\n",
        "                    clef = k ; min_length = len(k)            \n",
        "            category_keys.append(clef)\n",
        "            keywords_select[s] = clef\n",
        "        else:\n",
        "            category_keys.append(list(keywords_roots[s])[0])\n",
        "            keywords_select[s] = list(keywords_roots[s])[0]\n",
        "                   \n",
        "    print(\"Nb of keywords in variable '{}': {}\".format(colonne,len(category_keys)))\n",
        "    return category_keys, keywords_roots, keywords_select, count_keywords"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "f4da3052-c465-47bf-9652-a10a8ac51eb6",
        "_kg_hide-input": true,
        "_uuid": "1239a65ae122b1c020db626e5167451a950d8226",
        "colab_type": "code",
        "id": "jMprgkF-ww3-",
        "colab": {}
      },
      "source": [
        "df_produits = pd.DataFrame(df_initial['Description'].unique()).rename(columns = {0:'Description'})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "f52a4134-c9c7-4d17-8510-8f55b1530cbb",
        "_kg_hide-input": true,
        "_uuid": "38c4872616b2c40bf69982070165cc9db3d0ea69",
        "colab_type": "code",
        "id": "ANzTBDG4ww4C",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "keywords, keywords_roots, keywords_select, count_keywords = keywords_inventory(df_produits)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "e033781a-8038-4302-93ed-78966554b7cc",
        "_kg_hide-input": true,
        "_uuid": "66fb955b137916f16d838f95a6f5bbe5e4952334",
        "colab_type": "code",
        "id": "Pktzyl4gww4E",
        "colab": {}
      },
      "source": [
        "list_products = []\n",
        "for k,v in count_keywords.items():\n",
        "    list_products.append([keywords_select[k],v])\n",
        "list_products.sort(key = lambda x:x[1], reverse = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "d6c78812-343e-41af-8e4e-ef0292dc4f7e",
        "_kg_hide-input": true,
        "_uuid": "fcdf4d98e372a1d65c931b7a6d5c29f269938022",
        "colab_type": "code",
        "id": "olI4_-N7ww4G",
        "colab": {}
      },
      "source": [
        "liste = sorted(list_products, key = lambda x:x[1], reverse = True)\n",
        "#_______________________________\n",
        "plt.rc('font', weight='normal')\n",
        "fig, ax = plt.subplots(figsize=(7, 25))\n",
        "y_axis = [i[1] for i in liste[:125]]\n",
        "x_axis = [k for k,i in enumerate(liste[:125])]\n",
        "x_label = [i[0] for i in liste[:125]]\n",
        "plt.xticks(fontsize = 15)\n",
        "plt.yticks(fontsize = 13)\n",
        "plt.yticks(x_axis, x_label)\n",
        "plt.xlabel(\"Nb. of occurences\", fontsize = 18, labelpad = 10)\n",
        "ax.barh(x_axis, y_axis, align = 'center')\n",
        "ax = plt.gca()\n",
        "ax.invert_yaxis()\n",
        "#_______________________________________________________________________________________\n",
        "plt.title(\"Words occurence\",bbox={'facecolor':'k', 'pad':5}, color='w',fontsize = 25)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "e4170374-92b0-4063-9962-a80d36911891",
        "_uuid": "9308d23b2ccc25eb87f324f0da0616d768340739",
        "colab_type": "text",
        "id": "AYQDndq9ww4K"
      },
      "source": [
        "___\n",
        "### 3.2 Defining product categories "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "43300478-3b5a-4c7a-9466-34c384ccae60",
        "_kg_hide-input": true,
        "_uuid": "5f42482995f36f15688d8cba7f903e1277da5f92",
        "colab_type": "code",
        "id": "WLhLgma_ww4M",
        "colab": {}
      },
      "source": [
        "list_products = []\n",
        "for k,v in count_keywords.items():\n",
        "    word = keywords_select[k]\n",
        "    if word in ['pink', 'blue', 'tag', 'green', 'orange']: continue\n",
        "    if len(word) < 3 or v < 13: continue\n",
        "    if ('+' in word) or ('/' in word): continue\n",
        "    list_products.append([word, v])\n",
        "#______________________________________________________    \n",
        "list_products.sort(key = lambda x:x[1], reverse = True)\n",
        "print('words preserved:', len(list_products))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "68fa97e5-a7c6-4d65-8da6-8fddff742991",
        "_uuid": "fb4589b360ac2ebefebb66f710a6ba757c0ea203",
        "colab_type": "text",
        "id": "ynBjkmbtww4j"
      },
      "source": [
        "\n",
        "#### 3.2.1 Data encoding\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "23c77363-438b-4694-9b52-960c0ab3aa82",
        "_kg_hide-input": true,
        "_uuid": "d6faac7eb01d2251fb221569b75e007c8d5146aa",
        "colab_type": "code",
        "id": "gonTB_73ww4m",
        "colab": {}
      },
      "source": [
        "liste_produits = df_cleaned['Description'].unique()\n",
        "X = pd.DataFrame()\n",
        "for key, occurence in list_products:\n",
        "    X.loc[:, key] = list(map(lambda x:int(key.upper() in x), liste_produits))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "739c9cb1-3278-4d3a-a412-25dd2e8bc7c4",
        "_kg_hide-input": true,
        "_uuid": "b26a6bc55b61d0bcaccaf7c303b90d2a04636ac4",
        "colab_type": "code",
        "id": "bjDYwN2vww4q",
        "colab": {}
      },
      "source": [
        "threshold = [0, 1, 2, 3, 5, 10]\n",
        "label_col = []\n",
        "for i in range(len(threshold)):\n",
        "    if i == len(threshold)-1:\n",
        "        col = '.>{}'.format(threshold[i])\n",
        "    else:\n",
        "        col = '{}<.<{}'.format(threshold[i],threshold[i+1])\n",
        "    label_col.append(col)\n",
        "    X.loc[:, col] = 0\n",
        "\n",
        "for i, prod in enumerate(liste_produits):\n",
        "    prix = df_cleaned[ df_cleaned['Description'] == prod]['UnitPrice'].mean()\n",
        "    j = 0\n",
        "    while prix > threshold[j]:\n",
        "        j+=1\n",
        "        if j == len(threshold): break\n",
        "    X.loc[i, label_col[j-1]] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "8587b9a3-d3a6-41ed-ac8a-9e0cdc9af31e",
        "_kg_hide-input": true,
        "_uuid": "6ad9fc6ed72057c63594d9d97d89742202f064f7",
        "colab_type": "code",
        "id": "KtvK7XOtww4s",
        "colab": {}
      },
      "source": [
        "print(\"{:<8} {:<20} \\n\".format('range', 'nb. products') + 20*'-')\n",
        "for i in range(len(threshold)):\n",
        "    if i == len(threshold)-1:\n",
        "        col = '.>{}'.format(threshold[i])\n",
        "    else:\n",
        "        col = '{}<.<{}'.format(threshold[i],threshold[i+1])    \n",
        "    print(\"{:<10}  {:<20}\".format(col, X.loc[:, col].sum()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "026d2968-dbd2-46c4-bc39-24c147febb0b",
        "_uuid": "86e19d385e6849a17b262b627028e5aad7575b28",
        "colab_type": "text",
        "id": "3dW0Qgc8ww4v"
      },
      "source": [
        "____\n",
        "#### 3.2.2 Creating clusters of products\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "1c81a7c3-8980-4941-9082-7bf5cf92fc14",
        "_kg_hide-input": true,
        "_uuid": "4ce8586584935e81b9e403ea7a1dd4b2e4c9992e",
        "colab_type": "code",
        "id": "HJmTTNdrww4x",
        "colab": {}
      },
      "source": [
        "matrix = X.as_matrix()\n",
        "for n_clusters in range(3,10):\n",
        "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n",
        "    kmeans.fit(matrix)\n",
        "    clusters = kmeans.predict(matrix)\n",
        "    silhouette_avg = silhouette_score(matrix, clusters)\n",
        "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b29b3bb2-ef8d-4d7f-a19e-52045018ab8e",
        "_kg_hide-input": true,
        "_uuid": "72dd5bdab528264518bf034b84f66f3c29500fca",
        "colab_type": "code",
        "id": "UaOx4WsCww40",
        "colab": {}
      },
      "source": [
        "n_clusters = 5\n",
        "silhouette_avg = -1\n",
        "while silhouette_avg < 0.145:\n",
        "    kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=30)\n",
        "    kmeans.fit(matrix)\n",
        "    clusters = kmeans.predict(matrix)\n",
        "    silhouette_avg = silhouette_score(matrix, clusters)\n",
        "    print(\"For n_clusters =\", n_clusters, \"The average silhouette_score is :\", silhouette_avg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1f146c21-7d21-46d4-95f1-a788041957a0",
        "_uuid": "d211d2cc238a1d019469ba922081df9383b91d68",
        "colab_type": "text",
        "id": "V7QpaYMlww44"
      },
      "source": [
        "___\n",
        "#### 3.2.3  Characterizing the content of clusters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "ad66161e-87ba-42ca-8128-bda16b470a34",
        "_kg_hide-input": true,
        "_uuid": "83591dd72975afde6a85d70429add02bd278c125",
        "colab_type": "code",
        "id": "ZxPAP4r2ww45",
        "colab": {}
      },
      "source": [
        "pd.Series(clusters).value_counts()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c2b26710-2269-47e5-8f0d-98f6eac0015a",
        "_kg_hide-input": true,
        "_uuid": "e9e48bbbc3bb0ffa8134175aefb1fe808dea33e8",
        "colab_type": "code",
        "id": "znLgNxUBww48",
        "colab": {}
      },
      "source": [
        "def graph_component_silhouette(n_clusters, lim_x, mat_size, sample_silhouette_values, clusters):\n",
        "    plt.rcParams[\"patch.force_edgecolor\"] = True\n",
        "    plt.style.use('fivethirtyeight')\n",
        "    mpl.rc('patch', edgecolor = 'dimgray', linewidth=1)\n",
        "    #____________________________\n",
        "    fig, ax1 = plt.subplots(1, 1)\n",
        "    fig.set_size_inches(8, 8)\n",
        "    ax1.set_xlim([lim_x[0], lim_x[1]])\n",
        "    ax1.set_ylim([0, mat_size + (n_clusters + 1) * 10])\n",
        "    y_lower = 10\n",
        "    for i in range(n_clusters):\n",
        "        #___________________________________________________________________________________\n",
        "        # Aggregate the silhouette scores for samples belonging to cluster i, and sort them\n",
        "        ith_cluster_silhouette_values = sample_silhouette_values[clusters == i]\n",
        "        ith_cluster_silhouette_values.sort()\n",
        "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
        "        y_upper = y_lower + size_cluster_i\n",
        "        cmap = cm.get_cmap(\"Spectral\")\n",
        "        color = cmap(float(i) / n_clusters)        \n",
        "        ax1.fill_betweenx(np.arange(y_lower, y_upper), 0, ith_cluster_silhouette_values,\n",
        "                           facecolor=color, edgecolor=color, alpha=0.8)\n",
        "        #____________________________________________________________________\n",
        "        # Label the silhouette plots with their cluster numbers at the middle\n",
        "        ax1.text(-0.03, y_lower + 0.5 * size_cluster_i, str(i), color = 'red', fontweight = 'bold',\n",
        "                bbox=dict(facecolor='white', edgecolor='black', boxstyle='round, pad=0.3'))\n",
        "        #______________________________________\n",
        "        # Compute the new y_lower for next plot\n",
        "        y_lower = y_upper + 10  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "8ee417cb-29d9-4913-8e12-0c020ecde059",
        "_kg_hide-input": true,
        "_uuid": "cf9b40ed0d865401e4a05e06eec2bff97ace7059",
        "colab_type": "code",
        "id": "CVweDvjYww4_",
        "colab": {}
      },
      "source": [
        "#____________________________________\n",
        "# define individual silouhette scores\n",
        "sample_silhouette_values = silhouette_samples(matrix, clusters)\n",
        "#__________________\n",
        "# and do the graph\n",
        "graph_component_silhouette(n_clusters, [-0.07, 0.33], len(X), sample_silhouette_values, clusters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "159600b4-def2-4e4d-be54-6d84938721c4",
        "_kg_hide-input": true,
        "_uuid": "848009ae647f20366e148eeae96a4aa02f975618",
        "colab_type": "code",
        "id": "xsJuVEQPww5C",
        "colab": {}
      },
      "source": [
        "liste = pd.DataFrame(liste_produits)\n",
        "liste_words = [word for (word, occurence) in list_products]\n",
        "\n",
        "occurence = [dict() for _ in range(n_clusters)]\n",
        "\n",
        "for i in range(n_clusters):\n",
        "    liste_cluster = liste.loc[clusters == i]\n",
        "    for word in liste_words:\n",
        "        if word in ['art', 'set', 'heart', 'pink', 'blue', 'tag']: continue\n",
        "        occurence[i][word] = sum(liste_cluster.loc[:, 0].str.contains(word.upper()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "2995e126-925b-436f-8b68-55d874637b1e",
        "_kg_hide-input": true,
        "_uuid": "3d88a32f7998249ce42e267bb912acc7420f0c47",
        "colab_type": "code",
        "id": "Vi5T018Hww5H",
        "colab": {}
      },
      "source": [
        "#________________________________________________________________________\n",
        "def random_color_func(word=None, font_size=None, position=None,\n",
        "                      orientation=None, font_path=None, random_state=None):\n",
        "    h = int(360.0 * tone / 255.0)\n",
        "    s = int(100.0 * 255.0 / 255.0)\n",
        "    l = int(100.0 * float(random_state.randint(70, 120)) / 255.0)\n",
        "    return \"hsl({}, {}%, {}%)\".format(h, s, l)\n",
        "#________________________________________________________________________\n",
        "def make_wordcloud(liste, increment):\n",
        "    ax1 = fig.add_subplot(4,2,increment)\n",
        "    words = dict()\n",
        "    trunc_occurences = liste[0:150]\n",
        "    for s in trunc_occurences:\n",
        "        words[s[0]] = s[1]\n",
        "    #________________________________________________________\n",
        "    wordcloud = WordCloud(width=1000,height=400, background_color='lightgrey', \n",
        "                          max_words=1628,relative_scaling=1,\n",
        "                          color_func = random_color_func,\n",
        "                          normalize_plurals=False)\n",
        "    wordcloud.generate_from_frequencies(words)\n",
        "    ax1.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    ax1.axis('off')\n",
        "    plt.title('cluster nº{}'.format(increment-1))\n",
        "#________________________________________________________________________\n",
        "fig = plt.figure(1, figsize=(14,14))\n",
        "color = [0, 160, 130, 95, 280, 40, 330, 110, 25]\n",
        "for i in range(n_clusters):\n",
        "    list_cluster_occurences = occurence[i]\n",
        "\n",
        "    tone = color[i] # define the color of the words\n",
        "    liste = []\n",
        "    for key, value in list_cluster_occurences.items():\n",
        "        liste.append([key, value])\n",
        "    liste.sort(key = lambda x:x[1], reverse = True)\n",
        "    make_wordcloud(liste, i+1)            "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "85af75c5-bf90-4b8c-9687-caff723d0027",
        "_kg_hide-input": true,
        "_uuid": "33859d205bcf40477d166b679fb97996ba2d5f48",
        "colab_type": "code",
        "id": "i-vzMMwyww5K",
        "colab": {}
      },
      "source": [
        "pca = PCA()\n",
        "pca.fit(matrix)\n",
        "pca_samples = pca.transform(matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "41cd8738-4923-43cd-b26a-fba6d53070e8",
        "_kg_hide-input": true,
        "_uuid": "abce52d76e801aa6197603925618fb032aec78af",
        "colab_type": "code",
        "id": "5kVZNmGeww5Q",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(14, 5))\n",
        "sns.set(font_scale=1)\n",
        "plt.step(range(matrix.shape[1]), pca.explained_variance_ratio_.cumsum(), where='mid',\n",
        "         label='cumulative explained variance')\n",
        "sns.barplot(np.arange(1,matrix.shape[1]+1), pca.explained_variance_ratio_, alpha=0.5, color = 'g',\n",
        "            label='individual explained variance')\n",
        "plt.xlim(0, 100)\n",
        "\n",
        "ax.set_xticklabels([s if int(s.get_text())%2 == 0 else '' for s in ax.get_xticklabels()])\n",
        "\n",
        "plt.ylabel('Explained variance', fontsize = 14)\n",
        "plt.xlabel('Principal components', fontsize = 14)\n",
        "plt.legend(loc='upper left', fontsize = 13);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "0e49b019-d1b7-4815-8c5e-7cf27fa5b5b2",
        "_kg_hide-input": true,
        "_uuid": "fb0afe8523c2634860fdaf67d734c1dc0897c4c0",
        "colab_type": "code",
        "id": "ONjI6axIww5T",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=50)\n",
        "matrix_9D = pca.fit_transform(matrix)\n",
        "mat = pd.DataFrame(matrix_9D)\n",
        "mat['cluster'] = pd.Series(clusters)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "675cc670-4512-4983-8b58-e82f3cf2bf3d",
        "_kg_hide-input": true,
        "_uuid": "b9d872fa5038458f3424dfc585a5a823efc7ff7f",
        "colab_type": "code",
        "id": "uJ8sy7Sgww5W",
        "colab": {}
      },
      "source": [
        "import matplotlib.patches as mpatches\n",
        "\n",
        "sns.set_style(\"white\")\n",
        "sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2.5})\n",
        "\n",
        "LABEL_COLOR_MAP = {0:'r', 1:'gold', 2:'b', 3:'k', 4:'c', 5:'g'}\n",
        "label_color = [LABEL_COLOR_MAP[l] for l in mat['cluster']]\n",
        "\n",
        "fig = plt.figure(figsize = (15,8))\n",
        "increment = 0\n",
        "for ix in range(4):\n",
        "    for iy in range(ix+1, 4):    \n",
        "        increment += 1\n",
        "        ax = fig.add_subplot(2,3,increment)\n",
        "        ax.scatter(mat[ix], mat[iy], c= label_color, alpha=0.4) \n",
        "        plt.ylabel('PCA {}'.format(iy+1), fontsize = 12)\n",
        "        plt.xlabel('PCA {}'.format(ix+1), fontsize = 12)\n",
        "        ax.yaxis.grid(color='lightgray', linestyle=':')\n",
        "        ax.xaxis.grid(color='lightgray', linestyle=':')\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        \n",
        "        if increment == 9: break\n",
        "    if increment == 9: break\n",
        "        \n",
        "#_______________________________________________\n",
        "# I set the legend: abreviation -> airline name\n",
        "comp_handler = []\n",
        "for i in range(5):\n",
        "    comp_handler.append(mpatches.Patch(color = LABEL_COLOR_MAP[i], label = i))\n",
        "\n",
        "plt.legend(handles=comp_handler, bbox_to_anchor=(1.1, 0.97), \n",
        "           title='Cluster', facecolor = 'lightgrey',\n",
        "           shadow = True, frameon = True, framealpha = 1,\n",
        "           fontsize = 13, bbox_transform = plt.gcf().transFigure)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "731427f7-4c47-4d39-acb0-fb8b6efb0df7",
        "_uuid": "1d070f5bc58c75ac8bb122b5132761f81156f9ff",
        "colab_type": "text",
        "id": "a5Yonq5vww5b"
      },
      "source": [
        "___\n",
        "## 4. Customer categories\n",
        "\n",
        "### 4.1 Formatting data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "b66c0817-22c1-42cf-b944-8e3db918a671",
        "_kg_hide-input": true,
        "_uuid": "c030102cce28bbf7e268ed40b0388017cd15df8f",
        "colab_type": "code",
        "id": "LtgBBM_Iww5c",
        "colab": {}
      },
      "source": [
        "corresp = dict()\n",
        "for key, val in zip (liste_produits, clusters):\n",
        "    corresp[key] = val \n",
        "#__________________________________________________________________________\n",
        "df_cleaned['categ_product'] = df_cleaned.loc[:, 'Description'].map(corresp)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bec41c37-5777-49e3-98e0-4a6e9613783b",
        "_uuid": "738635fe9be465fbd0940e799b6070dbb201a780",
        "colab_type": "text",
        "id": "uV8b5TBgww5g"
      },
      "source": [
        "\n",
        "#### 4.1.1 Grouping products\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "882978a7-8d65-468e-982a-dd689515d415",
        "_kg_hide-input": true,
        "_uuid": "7b29ea4de78cdf757e031a0d810b24c4a3641057",
        "colab_type": "code",
        "id": "tyfRVKs-ww5i",
        "colab": {}
      },
      "source": [
        "for i in range(5):\n",
        "    col = 'categ_{}'.format(i)        \n",
        "    df_temp = df_cleaned[df_cleaned['categ_product'] == i]\n",
        "    price_temp = df_temp['UnitPrice'] * (df_temp['Quantity'] - df_temp['QuantityCanceled'])\n",
        "    price_temp = price_temp.apply(lambda x:x if x > 0 else 0)\n",
        "    df_cleaned.loc[:, col] = price_temp\n",
        "    df_cleaned[col].fillna(0, inplace = True)\n",
        "#__________________________________________________________________________________________________\n",
        "df_cleaned[['InvoiceNo', 'Description', 'categ_product', 'categ_0', 'categ_1', 'categ_2', 'categ_3','categ_4']][:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "9a5249e1-7ac8-43f9-a759-db4df4934e71",
        "_kg_hide-input": true,
        "_uuid": "9d0c9edb309503999f38ccc40fd09d1a311a5019",
        "colab_type": "code",
        "id": "t18scigWww5n",
        "colab": {}
      },
      "source": [
        "#___________________________________________\n",
        "# sum of purchases / user & order\n",
        "temp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['TotalPrice'].sum()\n",
        "basket_price = temp.rename(columns = {'TotalPrice':'Basket Price'})\n",
        "#____________________________________________________________\n",
        "# percentage of the price of the order / product category\n",
        "for i in range(5):\n",
        "    col = 'categ_{}'.format(i) \n",
        "    temp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)[col].sum()\n",
        "    basket_price.loc[:, col] = temp \n",
        "#_____________________\n",
        "# date of the order\n",
        "df_cleaned['InvoiceDate_int'] = df_cleaned['InvoiceDate'].astype('int64')\n",
        "temp = df_cleaned.groupby(by=['CustomerID', 'InvoiceNo'], as_index=False)['InvoiceDate_int'].mean()\n",
        "df_cleaned.drop('InvoiceDate_int', axis = 1, inplace = True)\n",
        "basket_price.loc[:, 'InvoiceDate'] = pd.to_datetime(temp['InvoiceDate_int'])\n",
        "#______________________________________\n",
        "# selection of significant entries:\n",
        "basket_price = basket_price[basket_price['Basket Price'] > 0]\n",
        "basket_price.sort_values('CustomerID', ascending = True)[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "893c7c56-0ef4-4055-965f-102e1d48bb80",
        "_uuid": "b60b304b08843db5b56367de6f1d366f243ec16c",
        "colab_type": "text",
        "id": "8gZ0yWuZww5r"
      },
      "source": [
        "#### 4.1.2 Separation of data over time\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c30fa4af-6617-4e25-b297-23efae5e1dcb",
        "_kg_hide-input": true,
        "_uuid": "7be642cc67d95c7b149747f66a8ba8845a17350e",
        "colab_type": "code",
        "id": "BLeWvCPUww5s",
        "colab": {}
      },
      "source": [
        "print(basket_price['InvoiceDate'].min(), '->',  basket_price['InvoiceDate'].max())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "29948719-cecf-48d3-ab66-ca76214b058a",
        "_kg_hide-input": true,
        "_uuid": "854a1781838e9d4f9e2a27e6a0b65c5e86d7a1b0",
        "colab_type": "code",
        "id": "JGfZs8v_ww5w",
        "colab": {}
      },
      "source": [
        "set_entrainement = basket_price[basket_price['InvoiceDate'] < datetime.date(2011,10,1)]\n",
        "set_test         = basket_price[basket_price['InvoiceDate'] >= datetime.date(2011,10,1)]\n",
        "basket_price = set_entrainement.copy(deep = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "cf755ead-5dee-4d55-b351-5c3d106ef828",
        "_uuid": "8f4ab5026a66fcdb1e6e62ee552a2737aebc61bb",
        "colab_type": "text",
        "id": "SviApxNTww51"
      },
      "source": [
        "____\n",
        "#### 4.1.3 Consumer Order Combinations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "876435e3-49c7-4f98-a58f-55d8f0620ef8",
        "_kg_hide-input": true,
        "_uuid": "a8144451cb606fde4e691486ea9b9d2ccbeae7b9",
        "colab_type": "code",
        "id": "bBF0pSQJww51",
        "colab": {}
      },
      "source": [
        "#________________________________________________________________\n",
        "# nb of visits and stats on the cart amount / users\n",
        "transactions_per_user=basket_price.groupby(by=['CustomerID'])['Basket Price'].agg(['count','min','max','mean','sum'])\n",
        "for i in range(5):\n",
        "    col = 'categ_{}'.format(i)\n",
        "    transactions_per_user.loc[:,col] = basket_price.groupby(by=['CustomerID'])[col].sum() /\\\n",
        "                                            transactions_per_user['sum']*100\n",
        "\n",
        "transactions_per_user.reset_index(drop = False, inplace = True)\n",
        "basket_price.groupby(by=['CustomerID'])['categ_0'].sum()\n",
        "transactions_per_user.sort_values('CustomerID', ascending = True)[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "60a47c3b-b36a-460a-835b-7f6d3c7af48c",
        "_kg_hide-input": true,
        "_uuid": "29dab7aeb0f6d8d1e898b9d72efa2a6e07f1d3e0",
        "colab_type": "code",
        "id": "9QBVtlcmww53",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "last_date = basket_price['InvoiceDate'].max().date()\n",
        "\n",
        "first_registration = pd.DataFrame(basket_price.groupby(by=['CustomerID'])['InvoiceDate'].min())\n",
        "last_purchase      = pd.DataFrame(basket_price.groupby(by=['CustomerID'])['InvoiceDate'].max())\n",
        "\n",
        "test  = first_registration.applymap(lambda x:(last_date - x.date()).days)\n",
        "test2 = last_purchase.applymap(lambda x:(last_date - x.date()).days)\n",
        "\n",
        "transactions_per_user.loc[:, 'LastPurchase'] = test2.reset_index(drop = False)['InvoiceDate']\n",
        "transactions_per_user.loc[:, 'FirstPurchase'] = test.reset_index(drop = False)['InvoiceDate']\n",
        "\n",
        "transactions_per_user[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "f0a8717e-bf13-4847-a2d8-d5c518ef7580",
        "_kg_hide-input": true,
        "_uuid": "27a45dcd6d0ac2382f07ef6b091a50fc7006a389",
        "colab_type": "code",
        "id": "ZXOZq5Howw55",
        "colab": {}
      },
      "source": [
        "n1 = transactions_per_user[transactions_per_user['count'] == 1].shape[0]\n",
        "n2 = transactions_per_user.shape[0]\n",
        "print(\"nb. customers with single purchase: {:<2}/{:<5} ({:<2.2f}%)\".format(n1,n2,n1/n2*100))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "0de7eb30-4a0c-49dc-a423-1acce0956e30",
        "_uuid": "f2c4d5aa8a322b09c539e6b3b0196ca7c4247104",
        "colab_type": "text",
        "id": "m4RYPXACww56"
      },
      "source": [
        "___\n",
        "### 4.2 Creation of customers categories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "93a76684-228f-447b-bad0-0a61809809cc",
        "_uuid": "5c58d53dcb2159e019c8eb55f0178f40ad293f90",
        "colab_type": "text",
        "id": "aFtOWkavww57"
      },
      "source": [
        "#### 4.2.1 Data encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "cdc3d67f-1337-4cf6-8a36-7b99d4adc160",
        "_kg_hide-input": true,
        "_uuid": "1769df5bd3f987760e0493c6979c9031e18cd47e",
        "colab_type": "code",
        "id": "0c8t8Fbjww57",
        "colab": {}
      },
      "source": [
        "list_cols = ['count','min','max','mean','categ_0','categ_1','categ_2','categ_3','categ_4']\n",
        "#_____________________________________________________________\n",
        "selected_customers = transactions_per_user.copy(deep = True)\n",
        "matrix = selected_customers[list_cols].as_matrix()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "84be0db9-a24a-4b9b-b541-e1437404ff37",
        "_kg_hide-input": true,
        "_uuid": "5e15c98b0da14a1fe5473ca2fa6c5722968386b6",
        "colab_type": "code",
        "id": "yQvRAFlgww59",
        "colab": {}
      },
      "source": [
        "scaler = StandardScaler()\n",
        "scaler.fit(matrix)\n",
        "print('variables mean values: \\n' + 90*'-' + '\\n' , scaler.mean_)\n",
        "scaled_matrix = scaler.transform(matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "302061b0-4e19-4b5b-a969-1f45c27873e1",
        "_kg_hide-input": true,
        "_uuid": "30b6a47f96e223efeb0ab49517b38c192627927f",
        "colab_type": "code",
        "id": "T7MA12MLww6A",
        "colab": {}
      },
      "source": [
        "pca = PCA()\n",
        "pca.fit(scaled_matrix)\n",
        "pca_samples = pca.transform(scaled_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "ed6ab99d-fee5-44c3-958c-58b786263e82",
        "_kg_hide-input": true,
        "_uuid": "bb95db84567cb2a3a32652bcf2b3ea5c14b5b550",
        "colab_type": "code",
        "id": "fRfIgtT8ww6E",
        "colab": {}
      },
      "source": [
        "fig, ax = plt.subplots(figsize=(14, 5))\n",
        "sns.set(font_scale=1)\n",
        "plt.step(range(matrix.shape[1]), pca.explained_variance_ratio_.cumsum(), where='mid',\n",
        "         label='cumulative explained variance')\n",
        "sns.barplot(np.arange(1,matrix.shape[1]+1), pca.explained_variance_ratio_, alpha=0.5, color = 'g',\n",
        "            label='individual explained variance')\n",
        "plt.xlim(0, 10)\n",
        "\n",
        "ax.set_xticklabels([s if int(s.get_text())%2 == 0 else '' for s in ax.get_xticklabels()])\n",
        "\n",
        "plt.ylabel('Explained variance', fontsize = 14)\n",
        "plt.xlabel('Principal components', fontsize = 14)\n",
        "plt.legend(loc='best', fontsize = 13);"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "25bea85a-ced6-402b-9e47-718ee3b6a434",
        "_uuid": "56151271895b2d34c5bfe02977169a47527bc72f",
        "colab_type": "text",
        "id": "jsyYltR_ww6G"
      },
      "source": [
        "___\n",
        "#### 4.2.2 Creation of customer categories"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "ce1adb5f-a0cf-4af4-99fa-585fa71cf89f",
        "_kg_hide-input": true,
        "_uuid": "09711facb0d6dd1e4027724a55b8a5fa0155b616",
        "colab_type": "code",
        "id": "E20qkTrmww6G",
        "colab": {}
      },
      "source": [
        "n_clusters = 11\n",
        "kmeans = KMeans(init='k-means++', n_clusters = n_clusters, n_init=100)\n",
        "kmeans.fit(scaled_matrix)\n",
        "clusters_clients = kmeans.predict(scaled_matrix)\n",
        "silhouette_avg = silhouette_score(scaled_matrix, clusters_clients)\n",
        "print('score of silhouette: {:<.3f}'.format(silhouette_avg))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "7e64fcb6-4827-4bb9-883e-f31e9c595dba",
        "_kg_hide-input": true,
        "_uuid": "b1f770e4ac40cc0e868efb12f895653fb5127599",
        "colab_type": "code",
        "id": "zlKPBAaBww6K",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "pd.DataFrame(pd.Series(clusters_clients).value_counts(), columns = ['nb. of clients']).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "051ee4e8-78a1-48d9-aa43-84e089a57df3",
        "_kg_hide-input": true,
        "_uuid": "716227b8a446ac77c9397893964c782333a02e6a",
        "colab_type": "code",
        "id": "TGlOWcZmww6N",
        "colab": {}
      },
      "source": [
        "pca = PCA(n_components=6)\n",
        "matrix_3D = pca.fit_transform(scaled_matrix)\n",
        "mat = pd.DataFrame(matrix_3D)\n",
        "mat['cluster'] = pd.Series(clusters_clients)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "6e2ff63a-1bfe-4c7a-b0e4-7fa387e2c560",
        "_kg_hide-input": true,
        "_uuid": "cececaffe4efaff3333c64426b028b793cc677b5",
        "colab_type": "code",
        "id": "KcF5Cu4zww6Q",
        "colab": {}
      },
      "source": [
        "import matplotlib.patches as mpatches\n",
        "\n",
        "sns.set_style(\"white\")\n",
        "sns.set_context(\"notebook\", font_scale=1, rc={\"lines.linewidth\": 2.5})\n",
        "\n",
        "LABEL_COLOR_MAP = {0:'r', 1:'tan', 2:'b', 3:'k', 4:'c', 5:'g', 6:'deeppink', 7:'skyblue', 8:'darkcyan', 9:'orange',\n",
        "                   10:'yellow', 11:'tomato', 12:'seagreen'}\n",
        "label_color = [LABEL_COLOR_MAP[l] for l in mat['cluster']]\n",
        "\n",
        "fig = plt.figure(figsize = (12,10))\n",
        "increment = 0\n",
        "for ix in range(6):\n",
        "    for iy in range(ix+1, 6):   \n",
        "        increment += 1\n",
        "        ax = fig.add_subplot(4,3,increment)\n",
        "        ax.scatter(mat[ix], mat[iy], c= label_color, alpha=0.5) \n",
        "        plt.ylabel('PCA {}'.format(iy+1), fontsize = 12)\n",
        "        plt.xlabel('PCA {}'.format(ix+1), fontsize = 12)\n",
        "        ax.yaxis.grid(color='lightgray', linestyle=':')\n",
        "        ax.xaxis.grid(color='lightgray', linestyle=':')\n",
        "        ax.spines['right'].set_visible(False)\n",
        "        ax.spines['top'].set_visible(False)\n",
        "        \n",
        "        if increment == 12: break\n",
        "    if increment == 12: break\n",
        "        \n",
        "#_______________________________________________\n",
        "# I set the legend: abreviation -> airline name\n",
        "comp_handler = []\n",
        "for i in range(n_clusters):\n",
        "    comp_handler.append(mpatches.Patch(color = LABEL_COLOR_MAP[i], label = i))\n",
        "\n",
        "plt.legend(handles=comp_handler, bbox_to_anchor=(1.1, 0.9), \n",
        "           title='Cluster', facecolor = 'lightgrey',\n",
        "           shadow = True, frameon = True, framealpha = 1,\n",
        "           fontsize = 13, bbox_transform = plt.gcf().transFigure)\n",
        "\n",
        "plt.tight_layout()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "89a83e55-2681-4c31-9e39-52fe2cd0e5cf",
        "_kg_hide-input": true,
        "_uuid": "3a207d8cfdc08bebeffb5cd8ddb00dfb959efb4a",
        "colab_type": "code",
        "id": "-EQH71pxww6U",
        "colab": {}
      },
      "source": [
        "sample_silhouette_values = silhouette_samples(scaled_matrix, clusters_clients)\n",
        "#____________________________________\n",
        "# define individual silouhette scores\n",
        "sample_silhouette_values = silhouette_samples(scaled_matrix, clusters_clients)\n",
        "#__________________\n",
        "# and do the graph\n",
        "graph_component_silhouette(n_clusters, [-0.15, 0.55], len(scaled_matrix), sample_silhouette_values, clusters_clients)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "1c45a7fd-b564-4595-b725-0d0ae03a25e7",
        "_uuid": "52f45c0a955a33a52c812aaf9e60e5a82f5bd2da",
        "colab_type": "code",
        "id": "CCuPMHWfww6X",
        "colab": {}
      },
      "source": [
        "selected_customers.loc[:, 'cluster'] = clusters_clients"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "edaa1b78-74be-44cf-952d-f5148824b5d8",
        "_kg_hide-input": true,
        "_uuid": "0fb5e92a263ea5290cfe72fb03257ea6eff632ce",
        "colab_type": "code",
        "id": "jbsBa7qlww6a",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "merged_df = pd.DataFrame()\n",
        "for i in range(n_clusters):\n",
        "    test = pd.DataFrame(selected_customers[selected_customers['cluster'] == i].mean())\n",
        "    test = test.T.set_index('cluster', drop = True)\n",
        "    test['size'] = selected_customers[selected_customers['cluster'] == i].shape[0]\n",
        "    merged_df = pd.concat([merged_df, test])\n",
        "#_____________________________________________________\n",
        "merged_df.drop('CustomerID', axis = 1, inplace = True)\n",
        "print('number of customers:', merged_df['size'].sum())\n",
        "\n",
        "merged_df = merged_df.sort_values('sum')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "9e510c5d-cfa4-4d50-ad2e-93fa01cb319e",
        "_kg_hide-input": true,
        "_uuid": "5c6079b1b9dcb0d894e406255d351680219a5b35",
        "colab_type": "code",
        "id": "UG1X9Bkpww6b",
        "colab": {}
      },
      "source": [
        "liste_index = []\n",
        "for i in range(5):\n",
        "    column = 'categ_{}'.format(i)\n",
        "    liste_index.append(merged_df[merged_df[column] > 45].index.values[0])\n",
        "#___________________________________\n",
        "liste_index_reordered = liste_index\n",
        "liste_index_reordered += [ s for s in merged_df.index if s not in liste_index]\n",
        "#___________________________________________________________\n",
        "merged_df = merged_df.reindex(index = liste_index_reordered)\n",
        "merged_df = merged_df.reset_index(drop = False)\n",
        "display(merged_df[['cluster', 'count', 'min', 'max', 'mean', 'sum', 'categ_0',\n",
        "                   'categ_1', 'categ_2', 'categ_3', 'categ_4', 'size']])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "e544d9cd-9589-46e8-9af5-3ad59a193b6b",
        "_kg_hide-input": true,
        "_uuid": "3d7b1bbb5feb7c4e5ab1e48a27104b5fa51cbac4",
        "colab_type": "code",
        "id": "cHpQoJO_ww6d",
        "colab": {}
      },
      "source": [
        "def _scale_data(data, ranges):\n",
        "    (x1, x2) = ranges[0]\n",
        "    d = data[0]\n",
        "    return [(d - y1) / (y2 - y1) * (x2 - x1) + x1 for d, (y1, y2) in zip(data, ranges)]\n",
        "\n",
        "class RadarChart():\n",
        "    def __init__(self, fig, location, sizes, variables, ranges, n_ordinate_levels = 6):\n",
        "\n",
        "        angles = np.arange(0, 360, 360./len(variables))\n",
        "\n",
        "        ix, iy = location[:] ; size_x, size_y = sizes[:]\n",
        "        \n",
        "        axes = [fig.add_axes([ix, iy, size_x, size_y], polar = True, \n",
        "        label = \"axes{}\".format(i)) for i in range(len(variables))]\n",
        "\n",
        "        _, text = axes[0].set_thetagrids(angles, labels = variables)\n",
        "        \n",
        "        for txt, angle in zip(text, angles):\n",
        "            if angle > -1 and angle < 181:\n",
        "                txt.set_rotation(angle - 90)\n",
        "            else:\n",
        "                txt.set_rotation(angle - 270)\n",
        "        \n",
        "        for ax in axes[1:]:\n",
        "            ax.patch.set_visible(False)\n",
        "            ax.xaxis.set_visible(False)\n",
        "            ax.grid(\"off\")\n",
        "        \n",
        "        for i, ax in enumerate(axes):\n",
        "            grid = np.linspace(*ranges[i],num = n_ordinate_levels)\n",
        "            grid_label = [\"\"]+[\"{:.0f}\".format(x) for x in grid[1:-1]]\n",
        "            ax.set_rgrids(grid, labels = grid_label, angle = angles[i])\n",
        "            ax.set_ylim(*ranges[i])\n",
        "        \n",
        "        self.angle = np.deg2rad(np.r_[angles, angles[0]])\n",
        "        self.ranges = ranges\n",
        "        self.ax = axes[0]\n",
        "                \n",
        "    def plot(self, data, *args, **kw):\n",
        "        sdata = _scale_data(data, self.ranges)\n",
        "        self.ax.plot(self.angle, np.r_[sdata, sdata[0]], *args, **kw)\n",
        "\n",
        "    def fill(self, data, *args, **kw):\n",
        "        sdata = _scale_data(data, self.ranges)\n",
        "        self.ax.fill(self.angle, np.r_[sdata, sdata[0]], *args, **kw)\n",
        "\n",
        "    def legend(self, *args, **kw):\n",
        "        self.ax.legend(*args, **kw)\n",
        "        \n",
        "    def title(self, title, *args, **kw):\n",
        "        self.ax.text(0.9, 1, title, transform = self.ax.transAxes, *args, **kw)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "d0066031-e494-407f-9e9f-b79167244fd1",
        "_kg_hide-input": true,
        "_uuid": "3074b64fa2e091118783baa4452fe7ccc22c82cb",
        "colab_type": "code",
        "id": "sqSuQmxAww6f",
        "colab": {}
      },
      "source": [
        "fig = plt.figure(figsize=(10,12))\n",
        "\n",
        "attributes = ['count', 'mean', 'sum', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4']\n",
        "ranges = [[0.01, 10], [0.01, 1500], [0.01, 10000], [0.01, 75], [0.01, 75], [0.01, 75], [0.01, 75], [0.01, 75]]\n",
        "index  = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
        "\n",
        "n_groups = n_clusters ; i_cols = 3\n",
        "i_rows = n_groups//i_cols\n",
        "size_x, size_y = (1/i_cols), (1/i_rows)\n",
        "\n",
        "for ind in range(n_clusters):\n",
        "    ix = ind%3 ; iy = i_rows - ind//3\n",
        "    pos_x = ix*(size_x + 0.05) ; pos_y = iy*(size_y + 0.05)            \n",
        "    location = [pos_x, pos_y]  ; sizes = [size_x, size_y] \n",
        "    #______________________________________________________\n",
        "    data = np.array(merged_df.loc[index[ind], attributes])    \n",
        "    radar = RadarChart(fig, location, sizes, attributes, ranges)\n",
        "    radar.plot(data, color = 'b', linewidth=2.0)\n",
        "    radar.fill(data, alpha = 0.2, color = 'b')\n",
        "    radar.title(title = 'cluster nº{}'.format(index[ind]), color = 'r')\n",
        "    ind += 1 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "f20f9253-85b2-4bd5-a3f1-c417e3ebd667",
        "_uuid": "9df02d92f07ba29fac28109e12b69e2a70fd43cf",
        "colab_type": "text",
        "id": "NJll0I-Mww6l"
      },
      "source": [
        "\n",
        "## 5. Classification of customers\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "13991b6e-7070-4e99-985a-8955cd995840",
        "_kg_hide-input": true,
        "_uuid": "33233378b919c14b0b43dd7bb8f5b7023ccb089a",
        "colab_type": "code",
        "id": "oI2vJxcaww6m",
        "colab": {}
      },
      "source": [
        "class Class_Fit(object):\n",
        "    def __init__(self, clf, params=None):\n",
        "        if params:            \n",
        "            self.clf = clf(**params)\n",
        "        else:\n",
        "            self.clf = clf()\n",
        "\n",
        "    def train(self, x_train, y_train):\n",
        "        self.clf.fit(x_train, y_train)\n",
        "\n",
        "    def predict(self, x):\n",
        "        return self.clf.predict(x)\n",
        "    \n",
        "    def grid_search(self, parameters, Kfold):\n",
        "        self.grid = GridSearchCV(estimator = self.clf, param_grid = parameters, cv = Kfold)\n",
        "        \n",
        "    def grid_fit(self, X, Y):\n",
        "        self.grid.fit(X, Y)\n",
        "        \n",
        "    def grid_predict(self, X, Y):\n",
        "        self.predictions = self.grid.predict(X)\n",
        "        print(\"Precision: {:.2f} % \".format(100*metrics.accuracy_score(Y, self.predictions)))\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "5bdd768a-8cb4-49dc-84fc-33c8b84a3362",
        "_kg_hide-input": true,
        "_uuid": "df64b250e989c1a33fe31921ee308056fc5a57b5",
        "colab_type": "code",
        "id": "N--NFncqww6n",
        "colab": {}
      },
      "source": [
        "columns = ['mean', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4' ]\n",
        "X = selected_customers[columns]\n",
        "Y = selected_customers['cluster']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c9c4174c-bd28-47df-9919-e951f45bd7f2",
        "_kg_hide-input": true,
        "_uuid": "43c2d31561df475b5c56f5123c3d1080ed3990a2",
        "colab_type": "code",
        "id": "hycFa9UIww6q",
        "colab": {}
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = model_selection.train_test_split(X, Y, train_size = 0.8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "993f99bb-184f-4511-ac15-43de11249199",
        "_uuid": "07c5a612a0d6e812ae597ee5b6f0ab9c9c5c3e06",
        "colab_type": "text",
        "id": "H89YzfiPww6s"
      },
      "source": [
        "___\n",
        "### 5.1 Support Vector Machine Classifier (SVC)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "eb88bafc-1335-47e2-bc5d-469a551e54b6",
        "_kg_hide-input": true,
        "_uuid": "31ad3b4f07ec0b501f70909560735d21e3f5a8da",
        "colab_type": "code",
        "id": "QaoHNSFEww6s",
        "colab": {}
      },
      "source": [
        "svc = Class_Fit(clf = svm.LinearSVC)\n",
        "svc.grid_search(parameters = [{'C':np.logspace(-2,2,10)}], Kfold = 5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "f8f0b1ec-6464-4a34-924c-85810c03a8ad",
        "_kg_hide-input": true,
        "_uuid": "174508011f559d2be87e39718f6798090edef992",
        "colab_type": "code",
        "id": "815T5QcUww6u",
        "colab": {}
      },
      "source": [
        "svc.grid_fit(X = X_train, Y = Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "5031fd84-a7a4-4ce1-add7-03e4af50cb15",
        "_kg_hide-input": true,
        "_uuid": "314d70285360f73a73734e8583d1cb7e58c7f71d",
        "colab_type": "code",
        "id": "515_j843ww6w",
        "colab": {}
      },
      "source": [
        "svc.grid_predict(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "3d334a90-3134-4e66-babd-7f2ea02efb11",
        "_kg_hide-input": true,
        "_uuid": "50e9d977bebb38f3568c91ab1856061774d115c6",
        "colab_type": "code",
        "id": "L7fD53q5ww6y",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        print(\"Normalized confusion matrix\")\n",
        "    else:\n",
        "        print('Confusion matrix, without normalization')\n",
        "    #_________________________________________________\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=0)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    #_________________________________________________\n",
        "    fmt = '.2f' if normalize else 'd'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "    #_________________________________________________\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "eea554d8-a672-4f1e-a6cf-ef4bf6a6950b",
        "_kg_hide-input": true,
        "_uuid": "edb4632a50b983e80ab28fca93b80e9117d14fc6",
        "colab_type": "code",
        "id": "FccC9jGsww60",
        "colab": {}
      },
      "source": [
        "class_names = [i for i in range(11)]\n",
        "cnf_matrix = confusion_matrix(Y_test, svc.predictions) \n",
        "np.set_printoptions(precision=2)\n",
        "plt.figure(figsize = (8,8))\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize = False, title='Confusion matrix')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "1405c216-0a26-49b7-84eb-b7ca59e18751",
        "_uuid": "89bea5dbfef84d99e79457142bb29a8ec136cc5a",
        "colab_type": "text",
        "id": "rB2vlYfMww62"
      },
      "source": [
        "___\n",
        "#### 5.1.2 Learning curve\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "76b2a6bb-aa79-406f-95ff-48fb3946c5a8",
        "_kg_hide-input": true,
        "_uuid": "199d688e55c37ba527adb63c31a0c243a45ba8da",
        "colab_type": "code",
        "id": "eyPesOuRww63",
        "colab": {}
      },
      "source": [
        "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
        "                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 10)):\n",
        "    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n",
        "    plt.figure()\n",
        "    plt.title(title)\n",
        "    if ylim is not None:\n",
        "        plt.ylim(*ylim)\n",
        "    plt.xlabel(\"Training examples\")\n",
        "    plt.ylabel(\"Score\")\n",
        "    train_sizes, train_scores, test_scores = learning_curve(\n",
        "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
        "    train_scores_mean = np.mean(train_scores, axis=1)\n",
        "    train_scores_std = np.std(train_scores, axis=1)\n",
        "    test_scores_mean = np.mean(test_scores, axis=1)\n",
        "    test_scores_std = np.std(test_scores, axis=1)\n",
        "    plt.grid()\n",
        "\n",
        "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
        "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
        "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
        "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
        "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
        "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
        "\n",
        "    plt.legend(loc=\"best\")\n",
        "    return plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "5e33dcc9-e613-4098-b282-70cd63c59768",
        "_kg_hide-input": true,
        "_uuid": "b89f7776830508bb4a4424c8cdecbe9b6765f43c",
        "colab_type": "code",
        "id": "ysif9ihvww66",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "g = plot_learning_curve(svc.grid.best_estimator_,\n",
        "                        \"SVC learning curves\", X_train, Y_train, ylim = [1.01, 0.6],\n",
        "                        cv = 5,  train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5,\n",
        "                                                0.6, 0.7, 0.8, 0.9, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "naa5xYmdm5ML",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print('\\n')\n",
        "print(\"Precision, Recall, F1\")\n",
        "print('\\n')\n",
        "CR=classification_report(Y_test, svc.predictions)\n",
        "print(CR)\n",
        "print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5c0a6c67-93ed-4dd3-bc11-f3cb107ee9c2",
        "_uuid": "5f47f200bbc07f370f1a3e8940af87d35a4c08c7",
        "colab_type": "text",
        "id": "uWAgSC_aww68"
      },
      "source": [
        "\n",
        "\n",
        "___\n",
        "### 5.2 Logistic Regression\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "c37cd77b-f96d-483b-bb57-14db65f97039",
        "_kg_hide-input": true,
        "_uuid": "9ee59b5060327224a4356be75ee1221b4e9fb0ab",
        "colab_type": "code",
        "id": "mVfHm8UOww68",
        "colab": {}
      },
      "source": [
        "lr = Class_Fit(clf = linear_model.LogisticRegression)\n",
        "lr.grid_search(parameters = [{'C':np.logspace(-2,2,20)}], Kfold = 5)\n",
        "lr.grid_fit(X = X_train, Y = Y_train)\n",
        "lr.grid_predict(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "8c4c95f0-8116-4501-ae86-7df54fa6a8dd",
        "_kg_hide-input": true,
        "_uuid": "203206e691175e8ff82cf72cfb34e82ffd2acf0c",
        "colab_type": "code",
        "id": "2_zqy7F-ww6-",
        "colab": {}
      },
      "source": [
        "g = plot_learning_curve(lr.grid.best_estimator_, \"Logistic Regression learning curves\", X_train, Y_train,\n",
        "                        ylim = [1.01, 0.7], cv = 5, \n",
        "                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "M_cEWQZrnpSq",
        "colab": {}
      },
      "source": [
        "class_names = [i for i in range(11)]\n",
        "cnf_matrix = confusion_matrix(Y_test, lr.predictions) \n",
        "np.set_printoptions(precision=2)\n",
        "plt.figure(figsize = (8,8))\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize = False, title='Confusion matrix')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SxJa7WSqnpwe",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print('\\n')\n",
        "print(\"Precision, Recall, F1\")\n",
        "print('\\n')\n",
        "CR=classification_report(Y_test, lr.predictions)\n",
        "print(CR)\n",
        "print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "600918f4-6211-4f52-a6ec-82777e8ca37a",
        "_uuid": "6e74472ba1f136a34fcdddf01b19d2e9af9d96fa",
        "colab_type": "text",
        "id": "qk5Uh7NVww7A"
      },
      "source": [
        "### 5.3 k-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "93db008f-7525-4705-aa66-ca3e407bfa3f",
        "_kg_hide-input": true,
        "_uuid": "c898bc2cc957afd96dd317aecc160b23efe3c973",
        "colab_type": "code",
        "id": "5wgxYodCww7D",
        "colab": {}
      },
      "source": [
        "knn = Class_Fit(clf = neighbors.KNeighborsClassifier)\n",
        "knn.grid_search(parameters = [{'n_neighbors': np.arange(1,50,1)}], Kfold = 5)\n",
        "knn.grid_fit(X = X_train, Y = Y_train)\n",
        "knn.grid_predict(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "80b0d359-06f2-4f75-b394-c77eaefb6a37",
        "_kg_hide-input": true,
        "_uuid": "c10d7fca1e4070dd6a97e5d9f5879e3632d6e83c",
        "colab_type": "code",
        "id": "NWtoeynyww7F",
        "colab": {}
      },
      "source": [
        "g = plot_learning_curve(knn.grid.best_estimator_, \"Nearest Neighbors learning curves\", X_train, Y_train,\n",
        "                        ylim = [1.01, 0.7], cv = 5, \n",
        "                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AwgCjRDGoBZa",
        "colab": {}
      },
      "source": [
        "class_names = [i for i in range(11)]\n",
        "cnf_matrix = confusion_matrix(Y_test, knn.predictions) \n",
        "np.set_printoptions(precision=2)\n",
        "plt.figure(figsize = (8,8))\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize = False, title='Confusion matrix')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_AH20RQuoB16",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print('\\n')\n",
        "print(\"Precision, Recall, F1\")\n",
        "print('\\n')\n",
        "CR=classification_report(Y_test, knn.predictions)\n",
        "print(CR)\n",
        "print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "43089987-a289-407c-b6e7-82dd0b4f86fb",
        "_uuid": "ca3adb1bdc494a6b012b200a1f935031eaf4f5b8",
        "colab_type": "text",
        "id": "N_1_uP9Zww7J"
      },
      "source": [
        "### 5.4 Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "0a4cd7ef-f03c-4c7a-8b78-0dc26dab137b",
        "_kg_hide-input": true,
        "_uuid": "48dbd3e263d5b8113b8ef64677e6df4c20076a7c",
        "colab_type": "code",
        "id": "OCxSUUfMww7J",
        "colab": {}
      },
      "source": [
        "tr = Class_Fit(clf = tree.DecisionTreeClassifier)\n",
        "tr.grid_search(parameters = [{'criterion' : ['entropy', 'gini'], 'max_features' :['sqrt', 'log2']}], Kfold = 5)\n",
        "tr.grid_fit(X = X_train, Y = Y_train)\n",
        "tr.grid_predict(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "896f795a-1cbb-4d08-9019-a1ab6963fc5c",
        "_kg_hide-input": true,
        "_uuid": "66fdbf731275232a3df7ef73fe54aeeef8dd0c33",
        "colab_type": "code",
        "id": "B3ZcAqVOww7N",
        "colab": {}
      },
      "source": [
        "g = plot_learning_curve(tr.grid.best_estimator_, \"Decision tree learning curves\", X_train, Y_train,\n",
        "                        ylim = [1.01, 0.7], cv = 5, \n",
        "                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "CFYlyVBWocgl",
        "colab": {}
      },
      "source": [
        "class_names = [i for i in range(11)]\n",
        "cnf_matrix = confusion_matrix(Y_test, tr.predictions) \n",
        "np.set_printoptions(precision=2)\n",
        "plt.figure(figsize = (8,8))\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize = False, title='Confusion matrix')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "avkxOGGrocwP",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print('\\n')\n",
        "print(\"Precision, Recall, F1\")\n",
        "print('\\n')\n",
        "CR=classification_report(Y_test, tr.predictions)\n",
        "print(CR)\n",
        "print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "67fc7d71-2722-44ef-bf78-893f0e8116d1",
        "_uuid": "6d8db24aa70ef84521664c217b0f9fa731f9cbe1",
        "colab_type": "text",
        "id": "fld2xG5lww7P"
      },
      "source": [
        "### 5.5 Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "191ce09c-a67d-434e-99e2-f186d9b21095",
        "_kg_hide-input": true,
        "_uuid": "5fd4df1e26bd7cdf10dfcf245fddafff86790f8c",
        "colab_type": "code",
        "id": "ftJ5CvXyww7P",
        "colab": {}
      },
      "source": [
        "rf = Class_Fit(clf = ensemble.RandomForestClassifier)\n",
        "param_grid = {'criterion' : ['entropy', 'gini'], 'n_estimators' : [20, 40, 60, 80, 100],\n",
        "               'max_features' :['sqrt', 'log2']}\n",
        "rf.grid_search(parameters = param_grid, Kfold = 5)\n",
        "rf.grid_fit(X = X_train, Y = Y_train)\n",
        "rf.grid_predict(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "8ea75385-53a0-42d2-8f14-0b0d89032b41",
        "_kg_hide-input": true,
        "_uuid": "8bd7e9d31969f8b5fc548ad856012d3027faa9a5",
        "colab_type": "code",
        "id": "FPDRtVZtww7T",
        "colab": {}
      },
      "source": [
        "g = plot_learning_curve(rf.grid.best_estimator_, \"Random Forest learning curves\", X_train, Y_train,\n",
        "                        ylim = [1.01, 0.7], cv = 5, \n",
        "                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hgSLy77TouVi",
        "colab": {}
      },
      "source": [
        "class_names = [i for i in range(11)]\n",
        "cnf_matrix = confusion_matrix(Y_test, rf.predictions) \n",
        "np.set_printoptions(precision=2)\n",
        "plt.figure(figsize = (8,8))\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize = False, title='Confusion matrix')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qOHRj1a0ouw6",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print('\\n')\n",
        "print(\"Precision, Recall, F1\")\n",
        "print('\\n')\n",
        "CR=classification_report(Y_test, rf.predictions)\n",
        "print(CR)\n",
        "print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "5d7add5f-9247-4f59-a3ab-12a7a5fc269f",
        "_uuid": "ec49824cd052c1898730e7acb6ce684d63c9c22e",
        "colab_type": "text",
        "id": "iZ5S7y4iww7U"
      },
      "source": [
        "### 5.6 AdaBoost Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "91c38cf5-f042-47d8-b74e-f7a0f9718f26",
        "_kg_hide-input": true,
        "_uuid": "26780fd696188d6b3f8cd9dad2f187003148ee19",
        "colab_type": "code",
        "id": "0eYqNVOJww7V",
        "colab": {}
      },
      "source": [
        "ada = Class_Fit(clf = AdaBoostClassifier)\n",
        "param_grid = {'n_estimators' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]}\n",
        "ada.grid_search(parameters = param_grid, Kfold = 5)\n",
        "ada.grid_fit(X = X_train, Y = Y_train)\n",
        "ada.grid_predict(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "4d188502-2d63-49f6-96fc-51d08e381b39",
        "_kg_hide-input": true,
        "_uuid": "fbc49326d70e7ac314cc9c92f5baba69dc3cf7b8",
        "colab_type": "code",
        "id": "_KJ0jKhUww7Y",
        "colab": {}
      },
      "source": [
        "g = plot_learning_curve(ada.grid.best_estimator_, \"AdaBoost learning curves\", X_train, Y_train,\n",
        "                        ylim = [1.01, 0.4], cv = 5, \n",
        "                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mL1GBiKCpC7Z",
        "colab": {}
      },
      "source": [
        "class_names = [i for i in range(11)]\n",
        "cnf_matrix = confusion_matrix(Y_test, ada.predictions) \n",
        "np.set_printoptions(precision=2)\n",
        "plt.figure(figsize = (8,8))\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize = False, title='Confusion matrix')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "15_xkkvrpDRE",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print('\\n')\n",
        "print(\"Precision, Recall, F1\")\n",
        "print('\\n')\n",
        "CR=classification_report(Y_test, ada.predictions)\n",
        "print(CR)\n",
        "print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "bca75b42-7617-428a-8d56-b8ec68378cee",
        "_uuid": "376352a7039a8037ba7acbcb788b58cfc9b38e67",
        "colab_type": "text",
        "id": "6hTANSweww7b"
      },
      "source": [
        "### 5.7 Gradient Boosting Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "d3cd367f-d2e9-498d-bb82-c1137dd21fdf",
        "_kg_hide-input": true,
        "_uuid": "f85e385c75291843a7c8db847aee48070f60e7ee",
        "colab_type": "code",
        "id": "zaBzHttiww7c",
        "colab": {}
      },
      "source": [
        "gb = Class_Fit(clf = ensemble.GradientBoostingClassifier)\n",
        "param_grid = {'n_estimators' : [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]}\n",
        "gb.grid_search(parameters = param_grid, Kfold = 5)\n",
        "gb.grid_fit(X = X_train, Y = Y_train)\n",
        "gb.grid_predict(X_test, Y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "2110efe3-4466-4bcb-a74a-0950253d7f60",
        "_kg_hide-input": true,
        "_uuid": "eeca4c1c096b8a40ea910e2472d5fb6ca44ebb19",
        "colab_type": "code",
        "id": "Mka3EHBgww7d",
        "scrolled": true,
        "colab": {}
      },
      "source": [
        "g = plot_learning_curve(gb.grid.best_estimator_, \"Gradient Boosting learning curves\", X_train, Y_train,\n",
        "                        ylim = [1.01, 0.7], cv = 5, \n",
        "                        train_sizes = [0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "O-I7fVaBpN8O",
        "colab": {}
      },
      "source": [
        "class_names = [i for i in range(11)]\n",
        "cnf_matrix = confusion_matrix(Y_test, gb.predictions) \n",
        "np.set_printoptions(precision=2)\n",
        "plt.figure(figsize = (8,8))\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize = False, title='Confusion matrix')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "V4_zeTr2pOqh",
        "colab": {}
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "print('\\n')\n",
        "print(\"Precision, Recall, F1\")\n",
        "print('\\n')\n",
        "CR=classification_report(Y_test, gb.predictions)\n",
        "print(CR)\n",
        "print('\\n')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "47f0fdc5-2871-494f-9158-1a88c449a7d6",
        "_uuid": "609d7ed4c45f18359ad51745f03b2a686175b7ad",
        "colab_type": "text",
        "id": "fT9MAv0Dww7f"
      },
      "source": [
        "___\n",
        "### 5.8 Let's vote !\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "532dcc6f-33a1-4726-a07a-e4bb2ef0b396",
        "_kg_hide-input": true,
        "_uuid": "40c82b43dd34bb9b7685c76bea0f5ca7e761c1d4",
        "colab_type": "code",
        "id": "0lcJ9UZSww7f",
        "colab": {}
      },
      "source": [
        "rf_best  = ensemble.RandomForestClassifier(**rf.grid.best_params_)\n",
        "gb_best  = ensemble.GradientBoostingClassifier(**gb.grid.best_params_)\n",
        "svc_best = svm.LinearSVC(**svc.grid.best_params_)\n",
        "tr_best  = tree.DecisionTreeClassifier(**tr.grid.best_params_)\n",
        "knn_best = neighbors.KNeighborsClassifier(**knn.grid.best_params_)\n",
        "lr_best  = linear_model.LogisticRegression(**lr.grid.best_params_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "7d80dd22-3e03-4069-b27d-cfad906a5acd",
        "_kg_hide-input": true,
        "_uuid": "9aedb584c04d4291460c4d30d215e6b2a33d879c",
        "colab_type": "code",
        "id": "hzx8wa7Zww7h",
        "colab": {}
      },
      "source": [
        "votingC = ensemble.VotingClassifier(estimators=[('rf', rf_best),('gb', gb_best),\n",
        "                                                ('knn', knn_best)], voting='soft')    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "bdd0e288-9e46-477b-94a1-a4bfa7fd9b93",
        "_kg_hide-input": true,
        "_uuid": "a89f838c7552464dd07c477ec3d9fd1aae91718c",
        "colab_type": "code",
        "id": "4tHjd6Htww7i",
        "colab": {}
      },
      "source": [
        "votingC = votingC.fit(X_train, Y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "40d3aad9-7cdf-45b1-8848-863fd003dbe1",
        "_kg_hide-input": true,
        "_uuid": "d582d291f5e82fb666b2e7bc80acd2851c9d87c5",
        "colab_type": "code",
        "id": "EDghGqvuww7l",
        "colab": {}
      },
      "source": [
        "predictions = votingC.predict(X_test)\n",
        "print(\"Precision: {:.2f} % \".format(100*metrics.accuracy_score(Y_test, predictions)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_cell_guid": "35f9a337-973f-4b94-9f45-beb59fc6e518",
        "_uuid": "7a1fa3d722ee821ebbacd93282098168d095282c",
        "colab_type": "text",
        "collapsed": true,
        "id": "M5iNPCaaww7m"
      },
      "source": [
        "\n",
        "## 6. Testing predictions\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "3b0d4452-2d4d-4640-af80-ae4d95a18ebe",
        "_kg_hide-input": true,
        "_uuid": "230cb86d7b613e90c2e5a6b386779c8847d83d58",
        "colab_type": "code",
        "id": "M0b_9PJqww7n",
        "colab": {}
      },
      "source": [
        "basket_price = set_test.copy(deep = True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "a71444d2-fde7-422a-98c8-018d2ed8faff",
        "_kg_hide-input": true,
        "_uuid": "ad3b38fbda3b8da6f9ea86c0edbcf02002d1f277",
        "colab_type": "code",
        "id": "D7vmp-OAww7o",
        "colab": {}
      },
      "source": [
        "transactions_per_user=basket_price.groupby(by=['CustomerID'])['Basket Price'].agg(['count','min','max','mean','sum'])\n",
        "for i in range(5):\n",
        "    col = 'categ_{}'.format(i)\n",
        "    transactions_per_user.loc[:,col] = basket_price.groupby(by=['CustomerID'])[col].sum() /\\\n",
        "                                            transactions_per_user['sum']*100\n",
        "\n",
        "transactions_per_user.reset_index(drop = False, inplace = True)\n",
        "basket_price.groupby(by=['CustomerID'])['categ_0'].sum()\n",
        "\n",
        "#_______________________\n",
        "# Correcting time range\n",
        "transactions_per_user['count'] = 5 * transactions_per_user['count']\n",
        "transactions_per_user['sum']   = transactions_per_user['count'] * transactions_per_user['mean']\n",
        "\n",
        "transactions_per_user.sort_values('CustomerID', ascending = True)[:5]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "ad4089ab-f703-483c-8d03-65f3c282c49a",
        "_kg_hide-input": true,
        "_uuid": "926c8e08380d645553e2c939c1bffd485e52b02e",
        "colab_type": "code",
        "id": "4Bj0E5_Oww7p",
        "colab": {}
      },
      "source": [
        "list_cols = ['count','min','max','mean','categ_0','categ_1','categ_2','categ_3','categ_4']\n",
        "#_____________________________________________________________\n",
        "matrix_test = transactions_per_user[list_cols].as_matrix()\n",
        "scaled_test_matrix = scaler.transform(matrix_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "7627cb8b-d69c-45b2-9460-e37f3440a2cf",
        "_kg_hide-input": true,
        "_uuid": "25274ee1b5fcb9ada2496fef0479ed26b8c90fea",
        "colab_type": "code",
        "id": "qq8i9phXww7q",
        "colab": {}
      },
      "source": [
        "Y = kmeans.predict(scaled_test_matrix)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "dc376e15-18a7-45d8-a941-804945f13d0c",
        "_kg_hide-input": true,
        "_uuid": "db8446dff217a860253c13c56bb6e30e03270e54",
        "colab_type": "code",
        "id": "JXJRVvV1ww7r",
        "colab": {}
      },
      "source": [
        "columns = ['mean', 'categ_0', 'categ_1', 'categ_2', 'categ_3', 'categ_4' ]\n",
        "X = transactions_per_user[columns]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "1226d84c-0b7b-4d0d-b55a-c9c39c9f0d9a",
        "_kg_hide-input": true,
        "_uuid": "a77b2c4f0fba4a51a4d8915e3cd684e3afd3b6fa",
        "colab_type": "code",
        "id": "WctW0dVEww7s",
        "colab": {}
      },
      "source": [
        "classifiers = [(svc, 'Support Vector Machine'),\n",
        "                (lr, 'Logostic Regression'),\n",
        "                (knn, 'k-Nearest Neighbors'),\n",
        "                (tr, 'Decision Tree'),\n",
        "                (rf, 'Random Forest'),\n",
        "                (gb, 'Gradient Boosting')]\n",
        "#______________________________\n",
        "for clf, label in classifiers:\n",
        "    print(30*'_', '\\n{}'.format(label))\n",
        "    clf.grid_predict(X, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "13682ddc-d732-4839-a314-ec1bcd00cf2a",
        "_kg_hide-input": true,
        "_uuid": "fb82ea972aae88f12a6466a785c7be659984579c",
        "colab_type": "code",
        "id": "dJXy67-0ww7u",
        "colab": {}
      },
      "source": [
        "predictions = votingC.predict(X)\n",
        "print(\"Precision: {:.2f} % \".format(100*metrics.accuracy_score(Y, predictions)))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}